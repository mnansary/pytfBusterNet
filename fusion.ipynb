{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fusion.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnansary/pytfBusterNet/blob/master/fusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1J2CLKdouXg",
        "colab_type": "text"
      },
      "source": [
        "# colab specific task\n",
        "*   mount google drive\n",
        "*   change working directory to git repo\n",
        "*   Check TF version\n",
        "*    TPU check\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgzKSBAbolN8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a71f96e-dda1-4ba5-8f74-9b3df0bab755"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj55wOYuo53N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "be6b500a-030e-4568-aa2f-e97e55f834c3"
      },
      "source": [
        "cd /content/gdrive/My\\ Drive/CopyMove/pytfBusterNet/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/CopyMove/pytfBusterNet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv_z9Z-To64I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "1d32c5d5-1500-4785-91f2-f9b2fbb156ef"
      },
      "source": [
        "!pip3 install tensorflow==1.13.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.33.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.16.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (3.0.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.15.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgOvkV_6pBZY",
        "colab_type": "text"
      },
      "source": [
        "## TPU Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0LpiBYcpCU-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "6124a5a0-ee80-4711-a9d9-9b15ce8c35aa"
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "\n",
        "  with tf.Session(tpu_address) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(devices)\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.101.157.130:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 4761961009959105818),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 5638059082635466872),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 18016856909551166751),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6947801159749927165),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 17060972187671535414),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 16873963880719943249),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 1523699779303488709),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 14906596199184025070),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 18303067172786003934),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 11387980976858962598),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 16995379351701383568)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD9jqwS_pox6",
        "colab_type": "text"
      },
      "source": [
        "# Fusion Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGAXOp2OptHr",
        "colab_type": "text"
      },
      "source": [
        "## IMPORT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fuq1fO5-prhJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7e3581c3-4e1d-480a-f72f-2dc4aac944a0"
      },
      "source": [
        "# imports for fusion_net and loading\n",
        "from BusterNet.models import fusion_net\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# read data\n",
        "def readh5(d_path):\n",
        "    data=h5py.File(d_path, 'r')\n",
        "    data = np.array(data['data'])\n",
        "    return data\n",
        "\n",
        "# load data\n",
        "d_path=os.path.join(os.getcwd(),'DataSet')\n",
        "Xmp=os.path.join(d_path,'Xm.h5')\n",
        "Xsp=os.path.join(d_path,'Xs.h5')\n",
        "Yp=os.path.join(d_path,'Y.h5')\n",
        "\n",
        "Xm=readh5(Xmp)\n",
        "Xs=readh5(Xsp)\n",
        "Y=readh5(Yp)\n",
        "\n",
        "print(Xm.shape)\n",
        "print(Xs.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(672, 256, 256, 1)\n",
            "(672, 256, 256, 1)\n",
            "(672, 256, 256, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsTAgjGhqtYg",
        "colab_type": "text"
      },
      "source": [
        "## Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEHAeZScquMK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "c0d5e981-ae9c-4e75-94fa-ac0505a597c9"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "model=fusion_net()\n",
        "model.summary()\n",
        "model.compile(optimizer=Adam(lr=0.01), loss=tf.keras.losses.categorical_crossentropy)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 256, 256, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 256, 256, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 256, 256, 2)  0           input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 256, 256, 3)  9           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 256, 256, 3)  57          concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 256, 256, 3)  153         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 256, 256, 9)  0           conv2d[0][0]                     \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1 (BatchNo (None, 256, 256, 9)  36          concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 256, 256, 9)  0           batch_normalization_v1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 256, 256, 3)  246         activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 256, 256, 3)  0           conv2d_3[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 501\n",
            "Trainable params: 483\n",
            "Non-trainable params: 18\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvK6hWhKq76V",
        "colab_type": "text"
      },
      "source": [
        "## Convert Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRTYqcO7q_TP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "0bfb7408-25c9-4176-c0cb-d6d52c97fa09"
      },
      "source": [
        "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "def convert_model_TPU(model):\n",
        "  return tf.contrib.tpu.keras_to_tpu_model(model,strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "model=convert_model_TPU(model)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.101.157.130:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 4761961009959105818)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 5638059082635466872)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 18016856909551166751)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6947801159749927165)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 17060972187671535414)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 16873963880719943249)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 1523699779303488709)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 14906596199184025070)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 18303067172786003934)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 11387980976858962598)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 16995379351701383568)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4h63-NErE3-",
        "colab_type": "text"
      },
      "source": [
        "## Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5re3IevrabJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs=250\n",
        "batch_size=30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUciP9F4rcn0",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjOg3cMPri4k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7f116f9-d4c7-429d-aab1-6d02f8fe5790"
      },
      "source": [
        "history=model.fit([Xs,Xm],Y,epochs=epochs,batch_size=batch_size, verbose=1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(3,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(3, 256, 256, 1), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(3, 256, 256, 1), dtype=tf.float32, name='input_2_10'), TensorSpec(shape=(3, 256, 256, 3), dtype=tf.float32, name='lambda_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Remapping placeholder for input_2\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f04d8583cc0> []\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 7.158541202545166 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "INFO:tensorflow:CPU -> TPU lr: 0.009999999776482582 {0.01}\n",
            "INFO:tensorflow:CPU -> TPU beta_1: 0.8999999761581421 {0.9}\n",
            "INFO:tensorflow:CPU -> TPU beta_2: 0.9990000128746033 {0.999}\n",
            "INFO:tensorflow:CPU -> TPU decay: 0.0 {0.0}\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "630/672 [===========================>..] - ETA: 1s - loss: 0.3346INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(1,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(1, 256, 256, 1), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(1, 256, 256, 1), dtype=tf.float32, name='input_2_10'), TensorSpec(shape=(1, 256, 256, 3), dtype=tf.float32, name='lambda_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Remapping placeholder for input_2\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f04d8583cc0> [<tf.Variable 'tpu_139658788042008/Adam/iterations:0' shape=() dtype=int64>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d7a8f550>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d7a8fe10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d7a50198>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d7a371d0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d79bc7b8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d79f5ac8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d79873c8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d78d1a58>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d7861f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d7828be0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d77f4978>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d77bca90>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d7787d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d76f12b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d774ef98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d765ee10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d764ff60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d75f3f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d755f860>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d7528e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d74f1470>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d745d5f8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d7429fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d73efe10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d7380c88>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d7324f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d72eefd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d72b6e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d7224be0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f04d71ec748>]\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 4.767093896865845 secs\n",
            "672/672 [==============================] - 31s 46ms/sample - loss: 0.3153\n",
            "Epoch 2/250\n",
            "672/672 [==============================] - 6s 9ms/sample - loss: 0.0170\n",
            "Epoch 3/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0157\n",
            "Epoch 4/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0150\n",
            "Epoch 5/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0144\n",
            "Epoch 6/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0144\n",
            "Epoch 7/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0127\n",
            "Epoch 8/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0152\n",
            "Epoch 9/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0134\n",
            "Epoch 10/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0147\n",
            "Epoch 11/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0143\n",
            "Epoch 12/250\n",
            "672/672 [==============================] - 6s 9ms/sample - loss: 0.0152\n",
            "Epoch 13/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0143\n",
            "Epoch 14/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0131\n",
            "Epoch 15/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0137\n",
            "Epoch 16/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0138\n",
            "Epoch 17/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0142\n",
            "Epoch 18/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0136\n",
            "Epoch 19/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0138\n",
            "Epoch 20/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0135\n",
            "Epoch 21/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0145\n",
            "Epoch 22/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0143\n",
            "Epoch 23/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0144\n",
            "Epoch 24/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0138\n",
            "Epoch 25/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0131\n",
            "Epoch 26/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0155\n",
            "Epoch 27/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0135\n",
            "Epoch 28/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0135\n",
            "Epoch 29/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0143\n",
            "Epoch 30/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0138\n",
            "Epoch 31/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0121\n",
            "Epoch 32/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0130\n",
            "Epoch 33/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0141\n",
            "Epoch 34/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0135\n",
            "Epoch 35/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0124\n",
            "Epoch 36/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0122\n",
            "Epoch 37/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0136\n",
            "Epoch 38/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0159\n",
            "Epoch 39/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0137\n",
            "Epoch 40/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0137\n",
            "Epoch 41/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0132\n",
            "Epoch 42/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0126\n",
            "Epoch 43/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0123\n",
            "Epoch 44/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0112\n",
            "Epoch 45/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0119\n",
            "Epoch 46/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0121\n",
            "Epoch 47/250\n",
            "672/672 [==============================] - 6s 9ms/sample - loss: 0.0111\n",
            "Epoch 48/250\n",
            "672/672 [==============================] - 6s 9ms/sample - loss: 0.0114\n",
            "Epoch 49/250\n",
            "672/672 [==============================] - 6s 9ms/sample - loss: 0.0120\n",
            "Epoch 50/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0111\n",
            "Epoch 51/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0121\n",
            "Epoch 52/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0119\n",
            "Epoch 53/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0117\n",
            "Epoch 54/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0111\n",
            "Epoch 55/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0119\n",
            "Epoch 56/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0118\n",
            "Epoch 57/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0119\n",
            "Epoch 58/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0126\n",
            "Epoch 59/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0106\n",
            "Epoch 60/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0118\n",
            "Epoch 61/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0124\n",
            "Epoch 62/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0123\n",
            "Epoch 63/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0124\n",
            "Epoch 64/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0118\n",
            "Epoch 65/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0113\n",
            "Epoch 66/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0120\n",
            "Epoch 67/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0122\n",
            "Epoch 68/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0119\n",
            "Epoch 69/250\n",
            "672/672 [==============================] - 6s 9ms/sample - loss: 0.0119\n",
            "Epoch 70/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0122\n",
            "Epoch 71/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0118\n",
            "Epoch 72/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0111\n",
            "Epoch 73/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0106\n",
            "Epoch 74/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0120\n",
            "Epoch 75/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0121\n",
            "Epoch 76/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0119\n",
            "Epoch 77/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0123\n",
            "Epoch 78/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0111\n",
            "Epoch 79/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0121\n",
            "Epoch 80/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0108\n",
            "Epoch 81/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0113\n",
            "Epoch 82/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0122\n",
            "Epoch 83/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0116\n",
            "Epoch 84/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0115\n",
            "Epoch 85/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0121\n",
            "Epoch 86/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0098\n",
            "Epoch 87/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0118\n",
            "Epoch 88/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0115\n",
            "Epoch 89/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0116\n",
            "Epoch 90/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0115\n",
            "Epoch 91/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0101\n",
            "Epoch 92/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0113\n",
            "Epoch 93/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0115\n",
            "Epoch 94/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0119\n",
            "Epoch 95/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0120\n",
            "Epoch 96/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0108\n",
            "Epoch 97/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0119\n",
            "Epoch 98/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0110\n",
            "Epoch 99/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0126\n",
            "Epoch 100/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0123\n",
            "Epoch 101/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0115\n",
            "Epoch 102/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0115\n",
            "Epoch 103/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0111\n",
            "Epoch 104/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0099\n",
            "Epoch 105/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0115\n",
            "Epoch 106/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0119\n",
            "Epoch 107/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0106\n",
            "Epoch 108/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0117\n",
            "Epoch 109/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0112\n",
            "Epoch 110/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0113\n",
            "Epoch 111/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0114\n",
            "Epoch 112/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0112\n",
            "Epoch 113/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0114\n",
            "Epoch 114/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0104\n",
            "Epoch 115/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0121\n",
            "Epoch 116/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0105\n",
            "Epoch 117/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0115\n",
            "Epoch 118/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0115\n",
            "Epoch 119/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0110\n",
            "Epoch 120/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0114\n",
            "Epoch 121/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0115\n",
            "Epoch 122/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0110\n",
            "Epoch 123/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0116\n",
            "Epoch 124/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0100\n",
            "Epoch 125/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0113\n",
            "Epoch 126/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0116\n",
            "Epoch 127/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0100\n",
            "Epoch 128/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0114\n",
            "Epoch 129/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0112\n",
            "Epoch 130/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0113\n",
            "Epoch 131/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0115\n",
            "Epoch 132/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0115\n",
            "Epoch 133/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0113\n",
            "Epoch 134/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0101\n",
            "Epoch 135/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0117\n",
            "Epoch 136/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0116\n",
            "Epoch 137/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0121\n",
            "Epoch 138/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0123\n",
            "Epoch 139/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0122\n",
            "Epoch 140/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0116\n",
            "Epoch 141/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0114\n",
            "Epoch 142/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0118\n",
            "Epoch 143/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0104\n",
            "Epoch 144/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0113\n",
            "Epoch 145/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0102\n",
            "Epoch 146/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0112\n",
            "Epoch 147/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0099\n",
            "Epoch 148/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0110\n",
            "Epoch 149/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0102\n",
            "Epoch 150/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0111\n",
            "Epoch 151/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0115\n",
            "Epoch 152/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0116\n",
            "Epoch 153/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0100\n",
            "Epoch 154/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0098\n",
            "Epoch 155/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0111\n",
            "Epoch 156/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0120\n",
            "Epoch 157/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0115\n",
            "Epoch 158/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0118\n",
            "Epoch 159/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0113\n",
            "Epoch 160/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0101\n",
            "Epoch 161/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0111\n",
            "Epoch 162/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0104\n",
            "Epoch 163/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0114\n",
            "Epoch 164/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0117\n",
            "Epoch 165/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0116\n",
            "Epoch 166/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0122\n",
            "Epoch 167/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0123\n",
            "Epoch 168/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0114\n",
            "Epoch 169/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0111\n",
            "Epoch 170/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0108\n",
            "Epoch 171/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0114\n",
            "Epoch 172/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0111\n",
            "Epoch 173/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0115\n",
            "Epoch 174/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0114\n",
            "Epoch 175/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0113\n",
            "Epoch 176/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0103\n",
            "Epoch 177/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0111\n",
            "Epoch 178/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0104\n",
            "Epoch 179/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0114\n",
            "Epoch 180/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0104\n",
            "Epoch 181/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0116\n",
            "Epoch 182/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0114\n",
            "Epoch 183/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0117\n",
            "Epoch 184/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0122\n",
            "Epoch 185/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0116\n",
            "Epoch 186/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0114\n",
            "Epoch 187/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0112\n",
            "Epoch 188/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0114\n",
            "Epoch 189/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0113\n",
            "Epoch 190/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0109\n",
            "Epoch 191/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0109\n",
            "Epoch 192/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0110\n",
            "Epoch 193/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0106\n",
            "Epoch 194/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0113\n",
            "Epoch 195/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0109\n",
            "Epoch 196/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0112\n",
            "Epoch 197/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0101\n",
            "Epoch 198/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0107\n",
            "Epoch 199/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0108\n",
            "Epoch 200/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0108\n",
            "Epoch 201/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0110\n",
            "Epoch 202/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0110\n",
            "Epoch 203/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0114\n",
            "Epoch 204/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0112\n",
            "Epoch 205/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0108\n",
            "Epoch 206/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0106\n",
            "Epoch 207/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0110\n",
            "Epoch 208/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0107\n",
            "Epoch 209/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0110\n",
            "Epoch 210/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0110\n",
            "Epoch 211/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0109\n",
            "Epoch 212/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0110\n",
            "Epoch 213/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0114\n",
            "Epoch 214/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0104\n",
            "Epoch 215/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0109\n",
            "Epoch 216/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0108\n",
            "Epoch 217/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0118\n",
            "Epoch 218/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0114\n",
            "Epoch 219/250\n",
            "672/672 [==============================] - 6s 9ms/sample - loss: 0.0111\n",
            "Epoch 220/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0107\n",
            "Epoch 221/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0107\n",
            "Epoch 222/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0105\n",
            "Epoch 223/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0109\n",
            "Epoch 224/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0104\n",
            "Epoch 225/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0099\n",
            "Epoch 226/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0111\n",
            "Epoch 227/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0108\n",
            "Epoch 228/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0104\n",
            "Epoch 229/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0104\n",
            "Epoch 230/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0105\n",
            "Epoch 231/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0107\n",
            "Epoch 232/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0107\n",
            "Epoch 233/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0093\n",
            "Epoch 234/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0092\n",
            "Epoch 235/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0108\n",
            "Epoch 236/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0094\n",
            "Epoch 237/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0095\n",
            "Epoch 238/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0108\n",
            "Epoch 239/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0097\n",
            "Epoch 240/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0106\n",
            "Epoch 241/250\n",
            "672/672 [==============================] - 6s 8ms/sample - loss: 0.0105\n",
            "Epoch 242/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0102\n",
            "Epoch 243/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0108\n",
            "Epoch 244/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0112\n",
            "Epoch 245/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0117\n",
            "Epoch 246/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0096\n",
            "Epoch 247/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0110\n",
            "Epoch 248/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0100\n",
            "Epoch 249/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0106\n",
            "Epoch 250/250\n",
            "672/672 [==============================] - 5s 8ms/sample - loss: 0.0105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7BBVuVuryXA",
        "colab_type": "text"
      },
      "source": [
        "## Save Model Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DheEY3klr16l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "dd9316a3-c327-4305-b616-3babf4039674"
      },
      "source": [
        "model.save_weights('fusion_net.h5')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLnYgdufr8jj",
        "colab_type": "text"
      },
      "source": [
        "## Plot Training Histoty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cFgKHr-svI0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "40e9412b-63cd-444f-c453-33d4f050c166"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model training')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f04d45412b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}