{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sim.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnansary/pytfBusterNet/blob/master/sim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ojVYZ7Spzpv",
        "colab_type": "text"
      },
      "source": [
        "# colab specific task\n",
        "*   mount google drive\n",
        "*   change working directory to git repo\n",
        "*   Check TF version\n",
        "*    TPU check\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--q4JaV2ps6z",
        "colab_type": "code",
        "outputId": "c0196024-05bd-457e-bdab-f47a28257749",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsVhQoAOqGGW",
        "colab_type": "code",
        "outputId": "ffed7ae0-147c-4f32-fbbe-87f49b91a981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/gdrive/My\\ Drive/CopyMove/pytfBusterNet/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/CopyMove/pytfBusterNet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vp4y0TiqKJh",
        "colab_type": "code",
        "outputId": "805f4991-ee7c-4f7b-a48b-d206f72a60de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!pip3 install tensorflow==1.13.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.7.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.33.6)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.16.5)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (41.2.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (3.0.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.15.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "692-uM-yqdaF",
        "colab_type": "text"
      },
      "source": [
        "## TPU Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT9QkSiJqed6",
        "colab_type": "code",
        "outputId": "019a4f06-fa26-4ae8-fa11-5f22c38e4376",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "\n",
        "  with tf.Session(tpu_address) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(devices)\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.105.178.82:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 18437138231593257713),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 1245047660100270366),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 2719730741179217368),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 12960076316537420554),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 13061412769639384113),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 393811878354663601),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 17349861516814511441),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 2796547196430024620),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 7685037092442483988),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 14805901467482259589),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 16290368487233703797)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxefiHZ4qlHA",
        "colab_type": "text"
      },
      "source": [
        "# Similiarity Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7UsHgSoquhJ",
        "colab_type": "text"
      },
      "source": [
        "## IMPORT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO3XPj7hqx2K",
        "colab_type": "code",
        "outputId": "e1d169f7-cb62-4745-c2dd-438113caeff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# imports for sim_net and loading\n",
        "from BusterNet.models import sim_net\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# read data\n",
        "def readh5(d_path):\n",
        "    data=h5py.File(d_path, 'r')\n",
        "    data = np.array(data['data'])\n",
        "    return data\n",
        "\n",
        "# load data\n",
        "d_path=os.path.join(os.getcwd(),'DataSet')\n",
        "Xp=os.path.join(d_path,'X.h5')\n",
        "Ysp=os.path.join(d_path,'Ys.h5')\n",
        "\n",
        "X=readh5(Xp)\n",
        "Ys=readh5(Ysp)\n",
        "print(X.shape)\n",
        "print(Ys.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(672, 256, 256, 3)\n",
            "(672, 256, 256, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blwtSzOarVYM",
        "colab_type": "text"
      },
      "source": [
        "## Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOro7D1krWYf",
        "colab_type": "code",
        "outputId": "60285c63-a1a3-4d9e-edac-97ce53b6772d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "model=sim_net()\n",
        "model.summary()\n",
        "model.compile(optimizer=Adam(lr=0.01), loss=tf.keras.losses.binary_crossentropy)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 256, 256, 64) 1792        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 256, 256, 64) 36928       conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 128, 128, 64) 0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 128, 128, 128 73856       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 128, 128, 128 147584      conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 128)  0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 64, 64, 256)  295168      max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 64, 64, 256)  590080      conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 64, 64, 256)  590080      conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 256)  0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 512)  1180160     max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 512)  2359808     conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 512)  2359808     conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 512)  0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 16, 16, 512)  0           max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 16, 16, 100)  0           lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1 (BatchNo (None, 16, 16, 100)  400         lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 32, 32, 100)  0           batch_normalization_v1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 64, 64, 100)  0           lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 128, 128, 100 0           lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 128, 128, 2)  202         lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 128, 128, 2)  1802        lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 128, 128, 2)  5002        lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 128, 128, 6)  0           conv2d_19[0][0]                  \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_4 (Batch (None, 128, 128, 6)  24          concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 128, 128, 6)  0           batch_normalization_v1_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 256, 256, 100 0           lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 256, 256, 6)  0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 256, 256, 106 0           lambda_7[0][0]                   \n",
            "                                                                 lambda_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 256, 256, 2)  214         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 256, 256, 2)  1910        concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 256, 256, 2)  5302        concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 256, 256, 6)  0           conv2d_22[0][0]                  \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_5 (Batch (None, 256, 256, 6)  24          concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 256, 256, 6)  0           batch_normalization_v1_5[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 256, 256, 106 0           lambda_7[0][0]                   \n",
            "                                                                 activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 256, 256, 2)  5302        concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 256, 256, 2)  10390       concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 256, 256, 2)  25654       concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 256, 256, 6)  0           conv2d_25[0][0]                  \n",
            "                                                                 conv2d_26[0][0]                  \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_6 (Batch (None, 256, 256, 6)  24          concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 256, 256, 6)  0           batch_normalization_v1_6[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 256, 256, 1)  55          activation_5[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 7,691,569\n",
            "Trainable params: 7,691,333\n",
            "Non-trainable params: 236\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fqLlsIcrgJ6",
        "colab_type": "text"
      },
      "source": [
        "## Convert Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLuMCkq5rha3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "6dadd861-e1f4-4729-aeeb-2f24ce4bd046"
      },
      "source": [
        "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "def convert_model_TPU(model):\n",
        "  return tf.contrib.tpu.keras_to_tpu_model(model,strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "model=convert_model_TPU(model)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.105.178.82:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 18437138231593257713)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 1245047660100270366)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 2719730741179217368)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 12960076316537420554)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 13061412769639384113)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 393811878354663601)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 17349861516814511441)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 2796547196430024620)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 7685037092442483988)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 14805901467482259589)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 16290368487233703797)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMWNlaqLNIPY",
        "colab_type": "text"
      },
      "source": [
        "## Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Eo7ajbNJOo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9b710bd5-51c2-4224-9eeb-b0fa53e86db6"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "epochs=250\n",
        "batch_size=30\n",
        "Xt,Xv,Yt,Yv = train_test_split(X,Ys, test_size=0.2)\n",
        "print(Xt.shape)\n",
        "print(Xv.shape)\n",
        "print(Yt.shape)\n",
        "print(Yv.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(537, 256, 256, 3)\n",
            "(135, 256, 256, 3)\n",
            "(537, 256, 256, 1)\n",
            "(135, 256, 256, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nQXx8YRNSyr",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWTUQlweNYYN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "273ab4bf-5f7a-4947-d3ba-ae18ad2657f1"
      },
      "source": [
        "history=model.fit(Xt,Yt,validation_data=(Xv,Yv),epochs=epochs,batch_size=batch_size, verbose=1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 537 samples, validate on 135 samples\n",
            "Epoch 1/250\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(3,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(3, 256, 256, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(3, 256, 256, 1), dtype=tf.float32, name='conv2d_28_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7fd0ebc5c710> []\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 135.87467408180237 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "INFO:tensorflow:CPU -> TPU lr: 0.009999999776482582 {0.01}\n",
            "INFO:tensorflow:CPU -> TPU beta_1: 0.8999999761581421 {0.9}\n",
            "INFO:tensorflow:CPU -> TPU beta_2: 0.9990000128746033 {0.999}\n",
            "INFO:tensorflow:CPU -> TPU decay: 0.0 {0.0}\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "510/537 [===========================>..] - ETA: 8s - loss: 0.3825 INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(3,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(3, 256, 256, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(3, 256, 256, 1), dtype=tf.float32, name='conv2d_28_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7fd0e3a1fe10> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 75.54681992530823 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(1,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(1, 256, 256, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(1, 256, 256, 1), dtype=tf.float32, name='conv2d_28_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7fd0e3a1fe10> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 88.03289914131165 secs\n",
            "537/537 [==============================] - 339s 631ms/sample - loss: 0.3710 - val_loss: 0.4567\n",
            "Epoch 2/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.1469 - val_loss: 0.4567\n",
            "Epoch 3/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.1362 - val_loss: 0.3638\n",
            "Epoch 4/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.1329 - val_loss: 0.2024\n",
            "Epoch 5/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.1293 - val_loss: 0.1687\n",
            "Epoch 6/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.1271 - val_loss: 0.1514\n",
            "Epoch 7/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.1267 - val_loss: 0.1474\n",
            "Epoch 8/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.1231 - val_loss: 0.1461\n",
            "Epoch 9/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.1267 - val_loss: 0.1406\n",
            "Epoch 10/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.1253 - val_loss: 0.1396\n",
            "Epoch 11/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.1193 - val_loss: 0.1353\n",
            "Epoch 12/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.1219 - val_loss: 0.1425\n",
            "Epoch 13/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.1155 - val_loss: 0.1358\n",
            "Epoch 14/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.1112 - val_loss: 0.1401\n",
            "Epoch 15/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.1101 - val_loss: 0.1383\n",
            "Epoch 16/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.1116 - val_loss: 0.1331\n",
            "Epoch 17/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.1089 - val_loss: 0.1280\n",
            "Epoch 18/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.1084 - val_loss: 0.1346\n",
            "Epoch 19/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.1029 - val_loss: 0.1221\n",
            "Epoch 20/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.1037 - val_loss: 0.1135\n",
            "Epoch 21/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.1026 - val_loss: 0.1344\n",
            "Epoch 22/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.1018 - val_loss: 0.1180\n",
            "Epoch 23/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0954 - val_loss: 0.1320\n",
            "Epoch 24/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0980 - val_loss: 0.1443\n",
            "Epoch 25/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0980 - val_loss: 0.1063\n",
            "Epoch 26/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0954 - val_loss: 0.1232\n",
            "Epoch 27/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0936 - val_loss: 0.1007\n",
            "Epoch 28/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0943 - val_loss: 0.1523\n",
            "Epoch 29/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0953 - val_loss: 0.0990\n",
            "Epoch 30/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0941 - val_loss: 0.1368\n",
            "Epoch 31/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0907 - val_loss: 0.1460\n",
            "Epoch 32/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0889 - val_loss: 0.0897\n",
            "Epoch 33/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0845 - val_loss: 0.0912\n",
            "Epoch 34/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0847 - val_loss: 0.0896\n",
            "Epoch 35/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0813 - val_loss: 0.0888\n",
            "Epoch 36/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0813 - val_loss: 0.0965\n",
            "Epoch 37/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0822 - val_loss: 0.1468\n",
            "Epoch 38/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0760 - val_loss: 0.2809\n",
            "Epoch 39/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0730 - val_loss: 0.0906\n",
            "Epoch 40/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0691 - val_loss: 0.1310\n",
            "Epoch 41/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0692 - val_loss: 0.1497\n",
            "Epoch 42/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0691 - val_loss: 0.3291\n",
            "Epoch 43/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0639 - val_loss: 0.1374\n",
            "Epoch 44/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0626 - val_loss: 0.1546\n",
            "Epoch 45/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0603 - val_loss: 0.0883\n",
            "Epoch 46/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0588 - val_loss: 0.0887\n",
            "Epoch 47/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0586 - val_loss: 0.1779\n",
            "Epoch 48/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0540 - val_loss: 0.0690\n",
            "Epoch 49/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0569 - val_loss: 0.1238\n",
            "Epoch 50/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0529 - val_loss: 0.0939\n",
            "Epoch 51/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0524 - val_loss: 0.2654\n",
            "Epoch 52/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0501 - val_loss: 0.1870\n",
            "Epoch 53/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0513 - val_loss: 0.0702\n",
            "Epoch 54/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0493 - val_loss: 0.1071\n",
            "Epoch 55/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0489 - val_loss: 0.0936\n",
            "Epoch 56/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0475 - val_loss: 0.1467\n",
            "Epoch 57/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0434 - val_loss: 0.0582\n",
            "Epoch 58/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0428 - val_loss: 0.1385\n",
            "Epoch 59/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0394 - val_loss: 0.2466\n",
            "Epoch 60/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0392 - val_loss: 0.3056\n",
            "Epoch 61/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0390 - val_loss: 0.1017\n",
            "Epoch 62/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0384 - val_loss: 0.0664\n",
            "Epoch 63/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0356 - val_loss: 0.0945\n",
            "Epoch 64/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0373 - val_loss: 0.0853\n",
            "Epoch 65/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0342 - val_loss: 0.0543\n",
            "Epoch 66/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0369 - val_loss: 0.0743\n",
            "Epoch 67/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0370 - val_loss: 0.0740\n",
            "Epoch 68/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0353 - val_loss: 0.1058\n",
            "Epoch 69/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0351 - val_loss: 0.0577\n",
            "Epoch 70/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0321 - val_loss: 0.0774\n",
            "Epoch 71/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0309 - val_loss: 0.0665\n",
            "Epoch 72/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0300 - val_loss: 0.0922\n",
            "Epoch 73/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0294 - val_loss: 0.1391\n",
            "Epoch 74/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0291 - val_loss: 0.0665\n",
            "Epoch 75/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0309 - val_loss: 0.1615\n",
            "Epoch 76/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0302 - val_loss: 0.0487\n",
            "Epoch 77/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0276 - val_loss: 0.0499\n",
            "Epoch 78/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0272 - val_loss: 0.0513\n",
            "Epoch 79/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0263 - val_loss: 0.0543\n",
            "Epoch 80/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0238 - val_loss: 0.0795\n",
            "Epoch 81/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0240 - val_loss: 0.1489\n",
            "Epoch 82/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0248 - val_loss: 0.0496\n",
            "Epoch 83/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0251 - val_loss: 0.0513\n",
            "Epoch 84/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0274 - val_loss: 0.0579\n",
            "Epoch 85/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0272 - val_loss: 0.0710\n",
            "Epoch 86/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0243 - val_loss: 0.0423\n",
            "Epoch 87/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0222 - val_loss: 0.0416\n",
            "Epoch 88/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0229 - val_loss: 0.0548\n",
            "Epoch 89/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0224 - val_loss: 0.0433\n",
            "Epoch 90/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0206 - val_loss: 0.0593\n",
            "Epoch 91/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0209 - val_loss: 0.0411\n",
            "Epoch 92/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0196 - val_loss: 0.0421\n",
            "Epoch 93/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0200 - val_loss: 0.0536\n",
            "Epoch 94/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0197 - val_loss: 0.0410\n",
            "Epoch 95/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0188 - val_loss: 0.0406\n",
            "Epoch 96/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0188 - val_loss: 0.0548\n",
            "Epoch 97/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.0189 - val_loss: 0.0483\n",
            "Epoch 98/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0187 - val_loss: 0.0408\n",
            "Epoch 99/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0188 - val_loss: 0.0402\n",
            "Epoch 100/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0173 - val_loss: 0.0503\n",
            "Epoch 101/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0165 - val_loss: 0.0385\n",
            "Epoch 102/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0188 - val_loss: 0.0427\n",
            "Epoch 103/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0214 - val_loss: 0.0422\n",
            "Epoch 104/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0202 - val_loss: 0.0439\n",
            "Epoch 105/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0203 - val_loss: 0.0493\n",
            "Epoch 106/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0179 - val_loss: 0.0437\n",
            "Epoch 107/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0168 - val_loss: 0.0390\n",
            "Epoch 108/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0156 - val_loss: 0.0428\n",
            "Epoch 109/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0153 - val_loss: 0.0417\n",
            "Epoch 110/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0150 - val_loss: 0.0393\n",
            "Epoch 111/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0159 - val_loss: 0.0389\n",
            "Epoch 112/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0172 - val_loss: 0.0393\n",
            "Epoch 113/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0161 - val_loss: 0.0424\n",
            "Epoch 114/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0156 - val_loss: 0.0394\n",
            "Epoch 115/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0150 - val_loss: 0.0413\n",
            "Epoch 116/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0149 - val_loss: 0.0403\n",
            "Epoch 117/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0151 - val_loss: 0.0384\n",
            "Epoch 118/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0143 - val_loss: 0.0395\n",
            "Epoch 119/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0141 - val_loss: 0.0394\n",
            "Epoch 120/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0154 - val_loss: 0.0486\n",
            "Epoch 121/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0145 - val_loss: 0.0423\n",
            "Epoch 122/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0145 - val_loss: 0.0392\n",
            "Epoch 123/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0144 - val_loss: 0.0389\n",
            "Epoch 124/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0140 - val_loss: 0.0391\n",
            "Epoch 125/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0132 - val_loss: 0.0397\n",
            "Epoch 126/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0133 - val_loss: 0.0405\n",
            "Epoch 127/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0152 - val_loss: 0.0418\n",
            "Epoch 128/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0143 - val_loss: 0.0395\n",
            "Epoch 129/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0140 - val_loss: 0.0514\n",
            "Epoch 130/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0139 - val_loss: 0.0449\n",
            "Epoch 131/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0141 - val_loss: 0.0404\n",
            "Epoch 132/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0128 - val_loss: 0.0362\n",
            "Epoch 133/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0128 - val_loss: 0.0396\n",
            "Epoch 134/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0132 - val_loss: 0.0423\n",
            "Epoch 135/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0134 - val_loss: 0.0444\n",
            "Epoch 136/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0126 - val_loss: 0.0377\n",
            "Epoch 137/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0125 - val_loss: 0.0412\n",
            "Epoch 138/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0126 - val_loss: 0.0390\n",
            "Epoch 139/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0117 - val_loss: 0.0392\n",
            "Epoch 140/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0116 - val_loss: 0.0448\n",
            "Epoch 141/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0116 - val_loss: 0.0445\n",
            "Epoch 142/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0113 - val_loss: 0.0397\n",
            "Epoch 143/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0114 - val_loss: 0.0441\n",
            "Epoch 144/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0123 - val_loss: 0.0401\n",
            "Epoch 145/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0123 - val_loss: 0.0406\n",
            "Epoch 146/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0118 - val_loss: 0.0472\n",
            "Epoch 147/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0113 - val_loss: 0.0373\n",
            "Epoch 148/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0113 - val_loss: 0.0434\n",
            "Epoch 149/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0109 - val_loss: 0.0428\n",
            "Epoch 150/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0115 - val_loss: 0.0422\n",
            "Epoch 151/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0121 - val_loss: 0.0399\n",
            "Epoch 152/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0113 - val_loss: 0.0390\n",
            "Epoch 153/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0113 - val_loss: 0.0399\n",
            "Epoch 154/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0114 - val_loss: 0.0381\n",
            "Epoch 155/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0116 - val_loss: 0.0403\n",
            "Epoch 156/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0111 - val_loss: 0.0405\n",
            "Epoch 157/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0108 - val_loss: 0.0438\n",
            "Epoch 158/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0104 - val_loss: 0.0399\n",
            "Epoch 159/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0102 - val_loss: 0.0391\n",
            "Epoch 160/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0106 - val_loss: 0.0552\n",
            "Epoch 161/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0110 - val_loss: 0.0468\n",
            "Epoch 162/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0103 - val_loss: 0.0462\n",
            "Epoch 163/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0099 - val_loss: 0.0514\n",
            "Epoch 164/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0103 - val_loss: 0.0414\n",
            "Epoch 165/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0100 - val_loss: 0.0436\n",
            "Epoch 166/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0099 - val_loss: 0.0529\n",
            "Epoch 167/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0104 - val_loss: 0.0607\n",
            "Epoch 168/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0107 - val_loss: 0.0442\n",
            "Epoch 169/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0109 - val_loss: 0.0439\n",
            "Epoch 170/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0108 - val_loss: 0.0563\n",
            "Epoch 171/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0107 - val_loss: 0.0519\n",
            "Epoch 172/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0103 - val_loss: 0.0474\n",
            "Epoch 173/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0098 - val_loss: 0.0435\n",
            "Epoch 174/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0096 - val_loss: 0.0464\n",
            "Epoch 175/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0092 - val_loss: 0.0606\n",
            "Epoch 176/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0090 - val_loss: 0.0474\n",
            "Epoch 177/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0092 - val_loss: 0.0664\n",
            "Epoch 178/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0094 - val_loss: 0.0449\n",
            "Epoch 179/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0098 - val_loss: 0.0558\n",
            "Epoch 180/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0106 - val_loss: 0.0413\n",
            "Epoch 181/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0113 - val_loss: 0.0441\n",
            "Epoch 182/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0111 - val_loss: 0.0404\n",
            "Epoch 183/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0102 - val_loss: 0.0380\n",
            "Epoch 184/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.0100 - val_loss: 0.0380\n",
            "Epoch 185/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0102 - val_loss: 0.0443\n",
            "Epoch 186/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0096 - val_loss: 0.0401\n",
            "Epoch 187/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.0090 - val_loss: 0.0387\n",
            "Epoch 188/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.0089 - val_loss: 0.0488\n",
            "Epoch 189/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.0088 - val_loss: 0.0435\n",
            "Epoch 190/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.0092 - val_loss: 0.0418\n",
            "Epoch 191/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.0093 - val_loss: 0.0460\n",
            "Epoch 192/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.0088 - val_loss: 0.0401\n",
            "Epoch 193/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.0085 - val_loss: 0.0376\n",
            "Epoch 194/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.0085 - val_loss: 0.0497\n",
            "Epoch 195/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.0085 - val_loss: 0.0398\n",
            "Epoch 196/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.0084 - val_loss: 0.0412\n",
            "Epoch 197/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0085 - val_loss: 0.0388\n",
            "Epoch 198/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.0096 - val_loss: 0.0419\n",
            "Epoch 199/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0094 - val_loss: 0.0457\n",
            "Epoch 200/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.0090 - val_loss: 0.0404\n",
            "Epoch 201/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0088 - val_loss: 0.0377\n",
            "Epoch 202/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0088 - val_loss: 0.0393\n",
            "Epoch 203/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0084 - val_loss: 0.0388\n",
            "Epoch 204/250\n",
            "537/537 [==============================] - 13s 25ms/sample - loss: 0.0082 - val_loss: 0.0460\n",
            "Epoch 205/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0087 - val_loss: 0.0456\n",
            "Epoch 206/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0093 - val_loss: 0.0560\n",
            "Epoch 207/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0090 - val_loss: 0.0432\n",
            "Epoch 208/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0083 - val_loss: 0.0395\n",
            "Epoch 209/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0079 - val_loss: 0.0431\n",
            "Epoch 210/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0091 - val_loss: 0.0445\n",
            "Epoch 211/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0097 - val_loss: 0.0390\n",
            "Epoch 212/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0090 - val_loss: 0.0400\n",
            "Epoch 213/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0084 - val_loss: 0.0440\n",
            "Epoch 214/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0098 - val_loss: 0.0506\n",
            "Epoch 215/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0118 - val_loss: 0.0428\n",
            "Epoch 216/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0099 - val_loss: 0.0475\n",
            "Epoch 217/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0099 - val_loss: 0.0499\n",
            "Epoch 218/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0095 - val_loss: 0.0508\n",
            "Epoch 219/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0084 - val_loss: 0.0452\n",
            "Epoch 220/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0079 - val_loss: 0.0500\n",
            "Epoch 221/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0076 - val_loss: 0.0434\n",
            "Epoch 222/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0078 - val_loss: 0.0429\n",
            "Epoch 223/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0077 - val_loss: 0.0441\n",
            "Epoch 224/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0078 - val_loss: 0.0381\n",
            "Epoch 225/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0077 - val_loss: 0.0373\n",
            "Epoch 226/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0078 - val_loss: 0.0375\n",
            "Epoch 227/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0087 - val_loss: 0.0403\n",
            "Epoch 228/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0085 - val_loss: 0.0386\n",
            "Epoch 229/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0085 - val_loss: 0.0507\n",
            "Epoch 230/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0079 - val_loss: 0.0479\n",
            "Epoch 231/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0076 - val_loss: 0.0381\n",
            "Epoch 232/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0075 - val_loss: 0.0460\n",
            "Epoch 233/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0071 - val_loss: 0.0416\n",
            "Epoch 234/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0070 - val_loss: 0.0416\n",
            "Epoch 235/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0071 - val_loss: 0.0390\n",
            "Epoch 236/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0072 - val_loss: 0.0391\n",
            "Epoch 237/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0074 - val_loss: 0.0436\n",
            "Epoch 238/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0075 - val_loss: 0.0498\n",
            "Epoch 239/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0078 - val_loss: 0.0393\n",
            "Epoch 240/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0072 - val_loss: 0.0428\n",
            "Epoch 241/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0070 - val_loss: 0.0419\n",
            "Epoch 242/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0071 - val_loss: 0.0417\n",
            "Epoch 243/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0077 - val_loss: 0.0384\n",
            "Epoch 244/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0073 - val_loss: 0.0422\n",
            "Epoch 245/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0076 - val_loss: 0.0385\n",
            "Epoch 246/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0074 - val_loss: 0.0447\n",
            "Epoch 247/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0075 - val_loss: 0.0421\n",
            "Epoch 248/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0073 - val_loss: 0.0431\n",
            "Epoch 249/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0071 - val_loss: 0.0509\n",
            "Epoch 250/250\n",
            "537/537 [==============================] - 13s 24ms/sample - loss: 0.0072 - val_loss: 0.0422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzx4-kyYbGfI",
        "colab_type": "text"
      },
      "source": [
        "## Save Model Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1iTz86KbNed",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "ae4087da-efec-45f9-a6b6-53082df1ac3c"
      },
      "source": [
        "model.save_weights('sim_net.h5')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1M9sjHIbSnf",
        "colab_type": "text"
      },
      "source": [
        "## Plot Training Histoty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndNSS7XIbXaK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "5067ed60-2ed2-4751-aa36-fac64e0b6817"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd0e28c9128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnXd4XMXV/z9nV71bcm/YxuCKsY0x\nhGoCJKaEDgYSAoTEbwiEkPaGkPwI6Z2QwgshlEBCCR1CTBwgpphiXHDv3XKRZdlWb7s7vz/m3t2r\n1Upay1prpT2f5/Gzu/fOnTuzK8/3nnNmzogxBkVRFEUB8HV3AxRFUZTkQUVBURRFCaOioCiKooRR\nUVAURVHCqCgoiqIoYVQUFEVRlDAqCkpKISJ/FZGfxFl2q4ick+g2KUoyoaKgKIqihFFRUJQeiIik\ndXcblN6JioKSdDhum2+LyHIRqRWRh0VkgIi8JiLVIvKGiPTxlL9IRFaJyEEReUtExnnOTRGRJc51\n/wCyou51oYgsda59X0QmxdnGC0TkYxGpEpEdInJ31PnTnPoOOudvcI5ni8hvRWSbiFSKyHzn2AwR\nKY3xPZzjvL9bRJ4Tkb+LSBVwg4hMF5EPnHvsFpE/iUiG5/oJIvK6iOwXkTIRuVNEBopInYiUeMpN\nFZFyEUmPp+9K70ZFQUlWLgfOBY4FPgO8BtwJ9MP+3d4GICLHAk8Btzvn5gD/FJEMZ4B8CfgbUAw8\n69SLc+0U4BHgf4AS4M/AKyKSGUf7aoHPA0XABcDNInKJU+9RTnv/6LRpMrDUue43wAnAKU6b/hcI\nxfmdXAw859zzCSAIfB3oC3wCOBv4itOGfOAN4N/AYGA08KYxZg/wFnCVp97rgKeNMc1xtkPpxago\nKMnKH40xZcaYncC7wAJjzMfGmAbgRWCKU24W8C9jzOvOoPYbIBs76J4MpAP3GmOajTHPAQs995gN\n/NkYs8AYEzTGPAY0Ote1izHmLWPMCmNMyBizHCtMZzqnrwXeMMY85dy3whizVER8wBeArxljdjr3\nfN8Y0xjnd/KBMeYl5571xpjFxpgPjTEBY8xWrKi5bbgQ2GOM+a0xpsEYU22MWeCcewz4HICI+IFr\nsMKpKCoKStJS5nlfH+NznvN+MLDNPWGMCQE7gCHOuZ2mZdbHbZ73RwHfdNwvB0XkIDDMua5dROQk\nEZnnuF0qgS9jn9hx6tgU47K+WPdVrHPxsCOqDceKyKsissdxKf0sjjYAvAyMF5GRWGus0hjzUSfb\npPQyVBSUns4u7OAOgIgIdkDcCewGhjjHXIZ73u8AfmqMKfL8yzHGPBXHfZ8EXgGGGWMKgQcA9z47\ngKNjXLMPaGjjXC2Q4+mHH+t68hKd0vh+YC1wjDGmAOte87ZhVKyGO9bWM1hr4TrUSlA8qCgoPZ1n\ngAtE5GwnUPpNrAvofeADIADcJiLpInIZMN1z7V+ALztP/SIiuU4AOT+O++YD+40xDSIyHesycnkC\nOEdErhKRNBEpEZHJjhXzCHCPiAwWEb+IfMKJYawHspz7pwPfBzqKbeQDVUCNiIwFbvacexUYJCK3\ni0imiOSLyEme848DNwAXoaKgeFBRUHo0xph12CfeP2KfxD8DfMYY02SMaQIuww5++7Hxhxc81y4C\nvgT8CTgAbHTKxsNXgB+JSDVwF1ac3Hq3A+djBWo/Nsh8vHP6W8AKbGxjP/BLwGeMqXTqfAhr5dQC\nLWYjxeBbWDGqxgrcPzxtqMa6hj4D7AE2AGd5zr+HDXAvMcZ4XWpKiiO6yY6ipCYi8l/gSWPMQ93d\nFiV5UFFQlBRERE4EXsfGRKq7uz1K8qDuI0VJMUTkMewahttVEJRo1FJQFEVRwqiloCiKooTpcUm1\n+vbta0aMGNHdzVAURelRLF68eJ8xJnrtSyt6nCiMGDGCRYsWdXczFEVRehQiEtfUY3UfKYqiKGFU\nFBRFUZQwKgqKoihKmB4XU4hFc3MzpaWlNDQ0dHdTegVZWVkMHTqU9HTdc0VRUo1eIQqlpaXk5+cz\nYsQIWibEVA4VYwwVFRWUlpYycuTI7m6OoihHmF7hPmpoaKCkpEQFoQsQEUpKStTqUpQUpVeIAqCC\n0IXod6koqUuvcB/FRWMNNFaD+CC3H/h6jR4qiqJ0GakzMjbVQs0eqN4FTTVdWvXBgwf5v//7v0O+\n7vzzz+fgwYNd2hZFUZTDIXVEIa8/9B3jfOjaJIBtiUIgEGj3ujlz5lBUVNSlbVEURTkcUsd9JGL/\nAZhQl1Z9xx13sGnTJiZPnkx6ejpZWVn06dOHtWvXsn79ei655BJ27NhBQ0MDX/va15g9ezYQSdlR\nU1PDeeedx2mnncb777/PkCFDePnll8nOzu7SdiqKonRErxOFH/5zFat3VcU+aULQXAdpNeCLv+vj\nBxfwg89MaPP8L37xC1auXMnSpUt56623uOCCC1i5cmV4SucjjzxCcXEx9fX1nHjiiVx++eWUlJS0\nqGPDhg089dRT/OUvf+Gqq67i+eef53Of+1zcbVQURekKep0otI87qyaxe0hMnz69xRz/P/zhD7z4\n4osA7Nixgw0bNrQShZEjRzJ58mQATjjhBLZu3ZrQNiqKosSi14lCe0/0BANQtgIKhkJehxlkO01u\nbm74/VtvvcUbb7zBBx98QE5ODjNmzIi5BiAzMzP83u/3U19fn7D2KYqitEXqBJohElOga2MK+fn5\nVFfH3tWwsrKSPn36kJOTw9q1a/nwww+79N6KoihdSa+zFNolHGjuWvdRSUkJp556KhMnTiQ7O5sB\nAwaEz82cOZMHHniAcePGMWbMGE4++eQuvbeiKEpX0uP2aJ42bZqJ3mRnzZo1jBs3ruOLjYHdSyFv\nABQMTlALewdxf6eKovQIRGSxMWZaR+VS0H3k63JLQVEUpbeQWqIAjjCoKCiKosQiBUXB1+WL1xRF\nUXoLKSgKou4jRVGUNkhRUVBLQVEUJRapJwoaaFYURWmT1BMFEbp68dqhkpeXB8CuXbu44oorYpaZ\nMWMG0VNvo7n33nupq6sLf9ZU3IqiHC4pKArJYykMHjyY5557rtPXR4uCpuJWFOVwSUFR6PqYwh13\n3MF9990X/nz33Xfzk5/8hLPPPpupU6dy3HHH8fLLL7e6buvWrUycOBGA+vp6rr76asaNG8ell17a\nIvfRzTffzLRp05gwYQI/+MEPAJtkb9euXZx11lmcddZZgE3FvW/fPgDuueceJk6cyMSJE7n33nvD\n9xs3bhxf+tKXmDBhAp/61Kc0x5KiKC3ofWkuXrsD9qxo+3yg3loK6Tnx1znwODjvF22enjVrFrff\nfju33HILAM888wxz587ltttuo6CggH379nHyySdz0UUXtbn/8f33309OTg5r1qxh+fLlTJ06NXzu\npz/9KcXFxQSDQc4++2yWL1/Obbfdxj333MO8efPo27dvi7oWL17Mo48+yoIFCzDGcNJJJ3HmmWfS\np08fTdGtKEq7pJ6lAHT14rUpU6awd+9edu3axbJly+jTpw8DBw7kzjvvZNKkSZxzzjns3LmTsrKy\nNut45513woPzpEmTmDRpUvjcM888w9SpU5kyZQqrVq1i9erV7bZn/vz5XHrppeTm5pKXl8dll13G\nu+++C2iKbkVR2qf3WQrtPNEDcGCb3aN5QDsptjvBlVdeyXPPPceePXuYNWsWTzzxBOXl5SxevJj0\n9HRGjBgRM2V2R2zZsoXf/OY3LFy4kD59+nDDDTd0qh4XTdGtKEp7pJ6lkKB1CrNmzeLpp5/mueee\n48orr6SyspL+/fuTnp7OvHnz2LZtW7vXn3HGGTz55JMArFy5kuXLlwNQVVVFbm4uhYWFlJWV8dpr\nr4WvaStl9+mnn85LL71EXV0dtbW1vPjii5x++uld2FtFUXorCRUFEZkpIutEZKOI3NFOuctFxIhI\nhxn8Dr9RiZl9NGHCBKqrqxkyZAiDBg3is5/9LIsWLeK4447j8ccfZ+zYse1ef/PNN1NTU8O4ceO4\n6667OOGEEwA4/vjjmTJlCmPHjuXaa6/l1FNPDV8ze/ZsZs6cGQ40u0ydOpUbbriB6dOnc9JJJ/HF\nL36RKVOmdHmfFUXpfSQsdbaI+IH1wLlAKbAQuMYYszqqXD7wLyADuNUY0+7k/MNKnQ1QuRNq98Hg\n4+PsSWqiqbMVpXeRDKmzpwMbjTGbjTFNwNPAxTHK/Rj4JdB5R/mh4C5eS5K1CoqiKMlEIkVhCLDD\n87nUORZGRKYCw4wx/2qvIhGZLSKLRGRReXn54bVK3C6rKCiKokTTbYFmEfEB9wDf7KisMeZBY8w0\nY8y0fv36tVUm3hu7F8TZ0tSjp+3GpyhK15FIUdgJDPN8Huocc8kHJgJvichW4GTglc4Em7Oysqio\nqIhzMHO6rJlSY2KMoaKigqysrO5uiqIo3UAi1yksBI4RkZFYMbgauNY9aYypBMJLcUXkLeBbHQWa\nYzF06FBKS0uJy7XUVAN1++HAWvD1vmUaXUFWVhZDhw7t7mYoitINJGxUNMYERORWYC7gBx4xxqwS\nkR8Bi4wxr3TVvdLT0xk5cmR8hZf9A+bOhq8ugZKju6oJiqIovYKEPiobY+YAc6KO3dVG2RmJbEuY\ntAz7Gmg8IrdTFEXpSaTeiuY0x1ceVFFQFEWJJgVFwcn9o5aCoihKK1JPFPyuKByZtXKKoig9idQT\nBdd9FGjq3nYoiqIkISkoCm6gWS0FRVGUaFJQFFxLQWMKiqIo0aSMKDz90XbO+s1bNJBuD+jsI0VR\nlFakjChUNTSzZV8tQXFEQd1HiqIorUgZUfD7bFcDPp2SqiiK0hapIwpOctSgT1c0K4qitEXqiILP\nqkLYfRTUKamKoijRpIwo+BxRCCGAQCjQvQ1SFEVJQlJGFPzO5jrBkAF/uoqCoihKDFJGFFxLIRgy\ndh+FYHM3t0hRFCX5SBlRSHPdR8aALx1CwW5ukaIoSvKRMqLgBpoDIQM+P4TUUlAURYkmZUTB58QU\nQhpTUBRFaZOUEYXwlFTjxhRUFBRFUaJJGVHweWcf+fxqKSiKosQgZUTBtRRCIZxAs4qCoihKNCkj\nCmnhQHPIuo800KwoitKKlBEFn3dKql+npCqKosQiZUQhsqIZG1PQxWuKoiitSBlRcDJnR1Y0a0xB\nURSlFSkjCq6lEFnR3ENFIdAEG97o7lYoitJLSR1RiM591FNFYcNceOJy2L+5u1uiKEovJDVFwd+D\nRaGpzr4213dvOxRF6ZWkpij05Cyprpj1VFFTFCWpSRlRCK9oNj3cfRQWBZ1SqyhK15MyohBZ0eyK\nQg8dVFUUFEVJICknChFLoae6jxwx6KmWjqIoSU3KiIKvt2zHqTEFRVESSMqIQlqrQHMPHVRdMTDq\nPlIUpetJGVHoNesU1FJQFCWBpIwotEiI1ytiCmopKIrS9SRUFERkpoisE5GNInJHjPNfFpEVIrJU\nROaLyPhEtaVlQrzeYCl0sSiULoKfD4faiq6tV1GUHkXCREFE/MB9wHnAeOCaGIP+k8aY44wxk4Ff\nAfckqj3hhHg9PXV2otxH5WuhsRJqyrq2XkVRehSJtBSmAxuNMZuNMU3A08DF3gLGmCrPx1zAJKox\naY4qhNztOHVFc0uaau2rBrAVJaVJS2DdQ4Adns+lwEnRhUTkFuAbQAbwyVgVichsYDbA8OHDO9UY\n130UCPXwLKmJch811bSsX1GUlKTbA83GmPuMMUcD3wG+30aZB40x04wx0/r169ep+7juo8iK5mYw\nCTNMEocrBl39RO9aCqFQ19arKEqPIpGisBMY5vk81DnWFk8DlySqMa1WNAOYHjgAJtp9pJaCoqQ0\niRSFhcAxIjJSRDKAq4FXvAVE5BjPxwuADYlqTMsVzY4o9MQBMGGi4LiPNKagKClNwmIKxpiAiNwK\nzAX8wCPGmFUi8iNgkTHmFeBWETkHaAYOANcnqj2tEuKBDTanZSbqlokhYTEFtRQURUlsoBljzBxg\nTtSxuzzvv5bI+3tpFWiGnjkAJmrxWlgU1FJQlFSm2wPNRwqfTxDxrGiGHioKiY4pqCgoSiqTMqIA\n1loIuusUoGeIQmVUbD5RotBYbV81pqAoKU1KiYLPJ5EVzZD8C9j2roXfjYddSyPHEpUlVWMKiqKQ\nYqLgF2kZaE72AbB2r/O6L3IsUZvsqPtIURRSTRR84iTEcwPNST4AupaMVwB09pGiKAkkBUUh5Ikp\nJLn76EiJgjGedQo9cEGfoihdRuqJQk+afRRqTxS6sO3N9YRzESb7d6IoSkJJKVHwieM+6imBZrd9\n3qByImIKruvIW7+iKClJSomC3xe1ojnZB8Cw+8jTTnMIi9cCTbD+Px2Xc11HoJaCoqQ4qSUK0ovc\nR/FMSd0wF568Eio2tV/OaynoOgVFSWlSShR8vugpqT3EfdTZmII72DdUxlcOkt96UhQloaSUKKT5\nxOY+8veQ3Eex3EeHIgpumUBD++VauI9UFBQllUkpUfBFzz4KJrkoxHQfHUJMwb2uub79ci0shST/\nThRFSSgpJQqRFc09JPdRu5bCIYhCh5aCxhQURbGklij43IR4Pcx9ZDrrPnKu69BS0NlHiqJYUkoU\nfCJRqbOTPNB8uIvXOhVT0BXNipLKpJQopPmjA81J7iqJOfvIaXM8bp5DiilI63spipJypJQo+KL3\nU0j2Fc2hw4wpuP3r0FKog4w8EJ/GFBQlxYlLFETkayJSIJaHRWSJiHwq0Y3ravy+aPdRkj8VH/aU\n1DhjCoEGu1e1Ly35vxNFURJKvJbCF4wxVcCngD7AdcAvEtaqBBHZec11HyW5pXC4WVLjdR8FG60o\niD/5XWqKoiSUeEXBcThzPvA3Y8wqz7Eeg8/nxFF7Su6jdtcpdGGgOdhs4yy+tOT/ThRFSSjxisJi\nEfkPVhTmikg+0OOmqYRTZ/t7mvvoMGcfdeg+agR/plVNjSkoSkqTFme5m4DJwGZjTJ2IFAM3Jq5Z\nicHv8xEIBT0rmnuI+8i78c0huY+cMnFZChkaU1AUJW5L4RPAOmPMQRH5HPB9oIMsa8mHX6JTZyf5\nABjtPjLm0LKkHlJMIUNjCoqixC0K9wN1InI88E1gE/B4wlqVIHrsiuawEMSwGNojFOeU1GCT4z5K\nU/eRoqQ48YpCwBhjgIuBPxlj7gPyE9esxBBZ0ewDpOeJQqzYQnvEHVNocgLNaikoSqoTb0yhWkS+\ni52KerqI+ID0xDUrMYQtBbCDYLKLQth95FgILUQhjjh/3DGFRsgqUFFQFCVuS2EW0Ihdr7AHGAr8\nOmGtShDh1NlgXSU9JdCcaEvBDTSLP/mFUlGUhBKXKDhC8ARQKCIXAg3GmB4XU0jzWgq+9OR/Km4l\nCjFWNrdHvOsUAo3O7CO/xhQUJcWJN83FVcBHwJXAVcACEbkikQ1LBOEVzeC4SpLcUoiefdRpSyGe\nQLNOSVUUJf6YwveAE40xewFEpB/wBvBcohqWCMJ7NEPPGACj91PwtjeeJ3p3Z7nmug7KNXmmpPa4\nNYmKonQh8cYUfK4gOFQcwrVJg188MYWeEGiOTogXK91FexyS+yjTsZ6S/DtRFCWhxGsp/FtE5gJP\nOZ9nAXMS06TE4fMJQfdB2OfveXs0u69pWYkJNGtMQVFSnrhEwRjzbRG5HDjVOfSgMebFxDUrMfh9\nEHTdI76eYClEiYFrHfgzD81SMMFI0ruY93FWNPcEl5qiKAklXksBY8zzwPMJbEvCSfP5PIHmtOQP\nNAeb7Gu0+ygtM3KuPbzC0VwfWxSMiQSaNc2FoqQ87cYFRKRaRKpi/KsWkaqOKheRmSKyTkQ2isgd\nMc5/Q0RWi8hyEXlTRI46nM50hF3R7H7oAWmio3dea+E+OgRLAdqOK7hxC9d9lOzfiaIoCaVdS8EY\n0+lUFiLiB+4DzgVKgYUi8ooxZrWn2MfANCfz6s3Ar7DxioRg3UduoLknLF6Ldh95LIVDiSlA23EF\n1+JIcwLN8VggiqL0WhI5g2g6sNEYs9kY0wQ8jc2dFMYYM88Y486X/BC7UjphtFjRnJ4LTTWJvN3h\nE3YfRcUU0jLjzJLqEb02LQXnHrpOQVEUEisKQ4Adns+lzrG2uAl4LdYJEZktIotEZFF5eXmnG+QX\nzzqF3BKo3dfpuo4IoTbWKcRtKQQJb5DXlqUQaLSvGlNQFIUkWWvg7NEwjTbyKRljHjTGTDPGTOvX\nr1+n7+P3CYGwKPSDuiQWBe/eCbGmpJqQLdMeoQBk5Nn3cVsKKgqKksokUhR2AsM8n4c6x1ogIudg\nV0xfZIxpTGB78PvsU3MoZCCnL9TtT95B0BvviDX7yHu8LUIByHTCQnHFFHQ7TkVJdRIpCguBY0Rk\npIhkAFcDr3gLiMgU4M9YQdgbo44uxS9WFILGQG5fwED9gUTftnN44wHRloI/s+XnNusIQOahWgoa\nU1CUVCZhomCMCQC3AnOBNcAzxphVIvIjEbnIKfZrIA94VkSWisgrbVTXJfgcSyEYMpBTYg8ma1wh\npqXgCTRDHKIQjLiPNKagKEocxL14rTMYY+YQlQ7DGHOX5/05ibx/NGH3UdhSIHnjCsFYloIrClkt\nj7dXR2YHohB2H6mloChKkgSajxRh95EbU4DktRRCccQUTAcZTb2B5rbWH7RwH/k7rlNRlF5NaomC\n133UUywFX1rsKanez20RCkB6dsv6ogm4oqBZUhVFSWVRCMcUKrqxRe0QKyNqOCFeRssybdYR9IhC\nR5ZCusYUFEVJLVEIB5qNsYNgVmESWwrOYJ2eHXudArQcwKvL4P7T4OD2yLFQANJzWtbX6j5OoDkt\nU2MKiqKklii4MYXw5mK5/ZI3puC6e9Ky21mn4BnA962DshVQvi5y7JDcR7qfgqIoqSYKTm/D+Y9y\n+iavpeAGmtOz2hEFzwDuTi0NeNb/hQI2ViD+iEUQja5oVhTFQ0qJgs+dfRR0U1307QGWQlY77qMY\nqbHdwT8UBIwd6P0Z8bmPxKeioCgpTkqJQprfE1MAG2yu7XyCvYTiikKLmELU4jUTw1IIRm3h6fM7\notCG+yi8n0K6xhQURUktUfB51ykAFA23otBY3Y2taoOQx1IwwZYJ8mKluXAtBVccwqKQZgf8tiyF\n8IrmTI0pKIqSWqLQYkUzQL8x9nXf+m5qUTt4LQWwi8raCzSH3UdRezD40trfvjPspnJiD/FkX1UU\npdeSWqIQbSn0G2tfy5NYFLzxg1YxBc/q47D7KGpfZ3+6Yym05T5qtLEEn98KiPfazmAMzP0elC7u\nfB2KonQbCc19lGy0SIgH0Gck+NLtdM5kIxRlKYQC7SfEa9NS8LcfaA40RtxRPl/kWn8n/zSCzfDB\nn+z6iKEndK4ORVG6jdS2FPxpUHJ0D7MUxD75u8dcwlNSm1peH5591E6g2V0h7VoKhxNXCEZZLIqi\n9ChSSxSiZx8B9D02OS2F6JhCKGhFwJcWGbzf+z1sftu+by+m0F6gOdhoM6SCjSl4r+0M0W4sRVF6\nFKklCuLZec2l3xjYv7nloq9EUb4O3vhhfIHcULSlEEMUNr4Oy/9h34cH41izj9pbp9DkcR91QUxB\nRUFRejSpJQrRMQWAARPtjJtVLyW+AWtegfn3xLfbW9hScHIXuTEFX5qNE7g01drX8JTUqECzKwqB\ntmIKTRF3lFvvYYlClMUSTXMDNNZ0vn5FURJKSomCT2K4j8ZeCMNOgjnfhh0LE9sAdz1EPOsigp40\nF2BFoeGg3TRHPKLQXGdfW80+6sB9tH+zU74xErh2ReGwYgrOfdoSobl3wlNXd75+RVESSkqJQk6G\nHfQq6zxBV38aXPpn61d/+Bx4+9ddczNjoCZq22lXDJqcJ+Vnrod5P4t9fXhHNEcUTBAqNkLx0RE3\nD0CTKwrtxRSi3Edlq+APU+y00WBzxFI4EjGFg9uhsrTz9SuKklBSShTGDsonK93Hgi37W54oHgm3\nfQxjLoB3f9tyMN/2AdQfPPSbrXsNfjsWKndGjoUthRpoqILVL8Hbv4x9fWO1XT+QWWA/hxxRKIkS\nhbgtBY8QVu2yrzVlUVNSj0BMoble4w2KksSklChkpvk5cUQx72+KkQQvMx8+9WM7YP37u1C9Bw5s\nhUfPg9f/H9TtP7QFWZv+a5/uvU/FXvfRpjfbv76x2rbJXS9QWw51FVAyumVMoTnKUoiZ5iLKUnDb\nEWhwAs3ulNQusBQ6mpLaXHdkgvqKonSKlBIFgFOO7sv6shrKq2MMTCVHw0lfhpXPwR+nOa4dA8uf\nhb9fDg99El66pe05/15KP7Kv9R6rxA2wNlVbSwKgeFTs6xurrZXgunTcfRJKRlsLwiXsPopan9Bi\n8VpmyzaHRaHRDt5p0esUOrFP89b37HfktkctBUXpkaScKJw62m7DOX9jG9lRZ/4MvvweiNjpniWj\nIVAPu5bA6HNh6d9h8V/bv0ljDexZad/XeUWhynmthg3/se+b69uoo8paCu5A7eZnKhkd5T6Kmn0U\nc0pqVKDZjWkEGuw/130kvpbXHgrb3oONb0Ct43prSziba9VSUJQkJuVEYcLgQkaU5PDzOWvZW9UQ\nu9DAidaVBHDO3TDyTDjm0/DZZ+GoU20coLHGWhIv32rLGQPP3gCb5lkBcWfweKefuk/oVbsix9ua\nnum6j1wBKF9nB+0+I6DPUXDT63DSzRFRiV7RHM6omh7DfVQTuSbQGJnhdDgxhYZK++r2q11LoVGT\n7ilKkpJyouD3CQ9cdwLVDQFufmIJTYE2XCVTr4eb37dTVj//MlzztLUezv2R9e//9yfw3h9g+TN2\nIK4pg1Uv2n87HNeR+KLcR44ouPsoFwy1T+2xBshYolB0VMTVM2w6ZBVYH30oFGP2UdQ6hRbuI8di\nCTTYQTrNWTV9ODGFsCg4Qfm2rAFXxNSFpChJScqJAsDYgQX85srjWbztAN97cUXLxWwuIjBggn0V\niSSLGzoNxl0EC+63bqVgI+xZboPSAHvXwM4l1s2TXdzSfeS6bVxRKBoOmEiw2EtYFJz7Vm63s6S8\nuAvbAvXtrGj2t+M+arTCEG0pdGadgis0YUshhvvIGM9iO3UhKUoykpKiAHDBpEHc9snRPLu4lFl/\n/oD75m2ksr6Z1buqeH11Wcvqd67CAAAgAElEQVRUGNGc/QMbAO7r7MdQujAiCuVrYfdSGDQZcooj\ng2SgKfI0f2CbfS0abl/dgdJLY5UNNHvjBzl9W5bJyHWur/NYCtGBZtdS8LhsvLOPmhsiloIcxorm\nBkcUGhxLIZYlEGgETNvnFUXpdlIqdXY03/jUGIYW53Dv6+v59dx1vL2+nA1l1Ryoa2bysCKenn0y\nWen+1hf2HQ3XPmP9+49fbN1F7oY9jVX23+DJULkj4j5q8sQOqpy1C0XDnGuqIa9/y3tEu48Asota\nlnGT5XmneQYarUi4cQNXFMBJiZ0eFVOo91gKhyEKYUuhHVHwWkRqKShKUpLSogBw1bRhXDVtGE99\ntJ3vvrCCvMw0vn7OsfzujfW89PFOJg4pZHT/vNbicMw59nXYidZScFNFuAyabKdpVpbCC7MjVgFE\n3DNhSyEq2BwM2AE02lLI7tOynOs+ao6yFJ69Hja8bj+7s4/AWZOQHrEUmmudKaldGFNoz1LwzrQK\ntBHkVxSlW0l5UXC5+sRh1DcFGTeogJNHFfP6mj38dM4aqhsCfO7k4fzkkuNiXzjsZBtc3vIu9BsH\n5Wvs8UGTrPuo9CPYuwoKh0VdKFAwxL6Ndh81OYN2Zn7LPEfRohB2H9V6Fo01Ou4px03jtRSCTUBu\npH53IE+LWtHcmZhCwyFaCuo+UpSkJGVjCtGICF84bSSfOLoEEeErM0ZT3RCgMDud5xfvbJkvycvY\n8+1rVSkMOQFy+9v8RFmFdhCvq7CLwQ46cQR3TUBWof0HraelNnpEwbt6OSvafeRYCt5pr4HGlpZH\nC0uhueX93AE8PTqm0AlLITrQHCshnrqPFCXpUVFog/OPG8RrXzudJ790EvXNQZ5auD12waLhMHiK\nfd9nBEz9PJxwvf0c/WQPkD8wci4jz75visqa2kIU4nAfuQOx+OzA3xgtCl5LwVO/6+pJi16ncIgr\nmr1BdNf66Mh9pJaCoiQl6j5qh3GDbDK600b35f/mbeSyKUPoX5AVo+BFsOtjKwqTrowczyluXbZg\niLUacoptGmxo7T5qy1KIDjRnOKLgTnvNzLd1ef31vrSIe8gdiJvasBR8nVzR7FoJQNhtFWq2s52c\ndOX2vp5+qqWgKEmJWgpx8MOLJ9AQCHH7P5bywpJSrnnwQ/67tixS4PhrYNQMGHFqywuzY4nCoMg5\nNybQlvsoqzBOS8EVhUI7oHtjAu46BbBWRLC59VN9tKVwqDEFt55ooq2BFpaCioKiJCNqKcTB0f3y\nuOvC8fzon6t5f1MFGX4fi/62ny+cOpKLJg9mwuBBdtVzNK6lUDAkMg01f1DkXEa+fR89+8gdZFtZ\nCm0Emr2WQjTR7iPvBj/ufQ43ptCeKHhnZbWYfaSioCjJiIpCnHzu5KM4b+JA1uyuZvzgAr7+j6U8\nPH8LT320nXe/80kKs9NbX+QO4qPPgSWP2ffhmEKxTYudltVaFLzuI+8A3SrQ7Azmbkwhq6B1G9zc\nRwAf/aXlwOxaBGmHsE6hud7ONMof4GlvVeyy0auaNdCsKElPQt1HIjJTRNaJyEYRuSPG+TNEZImI\nBETkikS2pSsoycvktGP6UpybwWNfmM5Lt5xKVUOAP765gXc3lFPTGPWEXTTcWgkTL7drATLyI5vm\nuFZERm77s4+8U1LdvEfhz64odGQpOIK14jlY8Yx9702/3SrQ3I4ovPtbeOjslsca2hCF6IFfp6Qq\nStKTMEtBRPzAfcC5QCmwUEReMcas9hTbDtwAfCtR7UgkE4cUMnPCQB6av4WH5m8hN8PPjLH9mX36\nKI4fVmRjAt9wuls03Fml7ASXXSsiI6+NQLNAem5kf4JY+Hw2rhB2H8WwFMQfsRSaPffJKbGJ/SCy\notkVivZiCvs32wV5oVAkMO1aCtHZWFvFFNRSUJRkJ5Huo+nARmPMZgAReRq4GAiLgjFmq3OuE7u6\nJAd3XzSB6SOLOaokh9dXlzF31R6Wbj/I29+eQZrf8zTeZ4RNexFtKWTmx3YfucnwfB38ROk5HVgK\nvogoeMnpGxEF1+JwLYa29ngAZ6tSY9vsuqvcmELeANtHl1buI52SqijJTiJFYQjgGSEoBU7qTEUi\nMhuYDTB8+PAOSh9ZBhZm8YXTbPbSs8cNYMaY/nz574v52Zy1LNl+AGMMt519DGd/6sfWzeIGh4tG\n2NeM3JbBX4iIArQMNMciPQfq2okpQMR95CW3L7j7DLmWgitU9ftbl3epdbYybazyiIJjKeT2ixKF\naPeRBpoVJdnpEVNSjTEPGmOmGWOm9evXr7ub0y7njh/A0D7ZPPLeFg7WNVHdGGD23xbz8LoMKksm\n82pZEb+d8ALzaobZTKyx3EfVuyJB5Y5EISPHkxajLVGIYSnkejKuhi2FTBv3qNtvg9dVu1tf51oX\n3jhCY5Xth+sacy2OaGugqdZOmwXNfaQoSUoiLYWdgDfhz1DnWK/G7xO+f8F45q7aw90XTSDNJ3zt\n6Y/58aur+fGr1nOW5hMCixdyzfTh/CwjF6nyfC21+2DLO3DyzfZzPO4jl3hEIavQuntyPeKa7lmQ\nl1NsU3PM+V/YuRi+ujiyAC0YsOeg5YyjBifNt9uWzHw76MdyH2UVQmOluo8UJUlJpCgsBI4RkZFY\nMbgauDaB90saZk4cyMyJA8Of//L5afxrxW42l9dy0shijh9WxD2vr+fBdzbz+WHNjCtfC49dBNO+\nANV77DTUSVfbizsSBdcdBS3dR7evjLhyvO6j079lrQ+vK8e1FMAGoOsq7Grn/ZvspkEDxttz9fsJ\nr1j2WgoNB+1g706RzcizFkWsQHNGjhUpdR8pSlKSMFEwxgRE5FZgLuAHHjHGrBKRHwGLjDGviMiJ\nwItAH+AzIvJDY8yERLWpuxARLpw0uMWxO2aOpayqgbdWpjEuDczeNcizTs6kARPtPtFgRaHvGDj9\nG7ErT/cM6G4cIi3b7tXg7tfg9ywgG3Q8jDoT3v+Tcy4jMosIIrOSXItg3ZyIKNTsjZTzWgpVu+xK\nba+lALGnpKbn2PaopaAoSUlCF68ZY+YAc6KO3eV5vxDrVko5fD7h3lmTeXLgt5k591Su+uQ5fKHP\nMtg8D8ZfGikoArd+1HZFwz8BZatshlZ3ZzbXt+/idR+5wWR3pXFaVC6nnBK7e1z1Hvt53WtwhjNj\n2I0nQEtRqN5thcMVH9eNFct9lJ5j11uopaAoSUmPCDT3VkSEa8+cxKBjT+Cn/97A1JcKeWn4nZEN\nfOLh9G/YtRCz/hYZ6DOiRcHjPsopsa/pUdNQXXL7OusQmm3cYeeiyOI6d+YRRNxHwWYrIAVDInW6\nlkIs91F6tmMpqCgoSjKiaS66GRHhl1dM4qF3t7BgcwXffm4Za/dUs6m8hv21Tfz9ppPIzuhgBpKL\nKwrtWQpukj5XDNKjLYViwnGDoSda91FNma2zNob7qKbMls8fZN1I0I4o1NuYQlqmWgqKkqSopZAE\n9M/P4s7zx/H4F05izMB8Hnh7Ex9vP8jibQd4fklp/BW5g39G1CI211JIz4mIQNh9lN2yrGtJAAx0\ndptzYwm15TbGkVUUsRTcaavxWApNTkxBRUFRkha1FJKIwpx0Xv3q6QAYY7j4vvd4ZP4Wrp0+HJ9P\nOriaiChEWwo+v0134R3w27QUYoiCG0uoLbcuJX8G1O2Dl26B4hH2XMFgO1vJe3+vKBhjg9dZRa3T\nYSiKkjSopZCkiAg3nTaSzftqeWj+5vguClsKebHPeVNvx2MpDHBmQNXuhY1vwO5lVhQyC2D7Alj6\nd3jvD7ZMweDWloJ3S87GKpt7qXCIWgqKksSoKCQxF04azHkTB/KzOWt5Zdmuji9Ia8NSACsK3p3g\n2rQUnBlMWYVQOAwQOLANnrgS9qywOZyyCuyqa7CDfVqWFZzwlFR39pFHFCqdBXoFg3VKqqIkMSoK\nSYzfJ9x79WSmHdWH7z6/nMXbDrBjfx2Vdc1tXODOPoqRGM+f3nInuI4shbyBdr+HnGLY8RGYEJz7\nY7j4vtYrpwsG26mz7cUU3CB0wRBnSqonzcVr34F5P4/dJ0VRjigqCklOZpqf318zhTS/j8vvf5/T\nfzWPk3/+Jit3xtjtzJ/R2iJwGXMejPbsg5AWFXB2yS4CJLKJTm5/u/80wMjTrZUQnXivYIh99a5o\nhihR8FgKaVkR11IoBEufhA3/afM7UBTlyKGi0AMYUpTNy7ecyj1XHc+vr5hEdoafX7y2tnVBfxpc\n/0848abW5y7+E0z5XOSzKwbpUZaCzwlIu9uG5vWLrCnoY7PBhi2FIdPsAF/grNZ2xSAj1wa2W1kK\nYuv1Z0TqrNhgXVA1nj2vlZ7F3rXw4IzIvh5Kj0ZnH/UQRvTNZURfm+eouiHAj15dzU9eXc3NM46m\nJM/ztD/85PgqDFsKWa3PXXyfjR2AtRTAup6yncytrnuo/1g449uRskNPhAt+CyNOt6ITbSnkDbBu\nLG+guXSRfa0pa7lxj9Jz2DzPWpO7l8HRZ3V3a5TDREWhB/LZk4ezYmclj7y3hddW7uGh66cxblAb\nGVLboi1LAWDMzMj7PEcUikdGjrnuo+KjW5b1+eHEL9r3/vSWs4+qdkUsCu+U1J2L7WsoYBPueVN6\nK91DdZndYe+oT7RfrnydtQgrNtrPB7YmvGlK4tHHsh5IZpqf382azMu3nEYgFOIzf5zP7U9/zIPv\nbKK+qZ2tNL20Zyl4cVNsF4/yNKCg9bFootcieEXBaynsXBTZBtTNt+Sy9Cl4+9ftt0/pev59B/zt\nkta5q6J5/ovw8legwlmfcnBb4tumJBy1FHowxw0t5NWvns4f3tzAq8t38dLSXZRVNZKV7qMkNzO8\nI1xM/JnWndPnqPZv4loKfUa2PtZvTPv1eweVqp0w8ozIuUCjzaVUtgqGnwLb5kPNHmBi5JpFD9un\n0DO/3X4bla6jocqmNgk0WGuh3xjY+KYV9P7jIuWaG2DvamdRpDOx4cA2K/4ZeW3vAngk2LvGujvd\nyRLKIaGWQg+nX34mP75kIh/f9SmumT6ch+dv4b55m/jxv1bz4eaKti/0+eBry2DK59u/gRtT8FoF\nx54HN77WcpCIxp8eCSbXH7DBZK+lEGyEJY9Zt9Gpt9nj1WU2FcYTV8GupfY/d/0BqHX6UbHJngfr\nunjsMy2T9Ckd01AJv5sI69uY7bX21ch04b1rIBSEZ663v0mjZy/xvavtbxdstFlyAQ5sgYfOhf98\nL7F9aI9QEP56Acz9bve1oYejotCL+M7MMYwZkM//nDmK4cU53PDoR1z1wAesL6tm496a1usb0rM7\nDuwOnmzTco84NXLMnwZHndL+da77qKkOXpgNCAxztuhOy7QDykcPwagZMOI0e7xmD5QuhA1z4d3f\nQJMzCFVssPtW338qvPd7e2ztv+wOdYseieOb6SZ2LW29/3Z3s3u53Xxp8aP2c3M9lC6OnF/xLBQM\nBcSmUN+72m73Wrkd3vyhp55lLevNKrTHqkphx8KEd6NNdi+z6VS2L+jc9R25zGJhTOfudSh4N7VK\nMCoKvYiinAzmfv0MvnveOB654USuPnE4m/fVcv7v3+Wce97mrN++xeMfbGX1rkP4A8vrD1/6LxQN\nP7TG+DPsorffTbBrEC78XSRw6Sboq94FJ91sp7Bm5NvEe7uW2HNrPdtw7NsAOxZAoB62vWePuYPS\nokci/5G3vAN/u6xlgBusq2Ptv7rmP29zfXz1NFTBw+fC3DsP/55dyd419nXjm7aNc78HD50NB3fY\nz1vehYmX2Rlle9fY3xBgzAXw0YOw7X37efcyKwTuCvhRM+wCR4B961vu7NcWtRVtl9uzAhY+fOj9\n2/yWfa0qjb3HuEtNOdwzwe4XArD1PfjL2fCTAfa7WfAgLH4svnu++GV49AJrhXXEiudg1Uvx1euy\n/j/wq5GR3y7BqCj0Uo7ul8fdF03glVtP5YJJg/jueWMZVJjFXS+v4vw/vMt1Dy+goiaB+Yeyi6xb\nYdhJ8IX/wLQbI+eGTLO7yV3+cGT2Uv4AG2h2ZyMZJ2DuS7ODjDsY7Vxi94revcy6tqp3W5cH2P/E\nm96E3UtbtuWdX8HT18KqFw6vT8311vXy7m87Llu60FpKy5/puvn7wWZY8vjh5Y3auxoQ6/aZ/zvr\nwsPY723Tm3YfjTHnWddg+VorCrn94bIHoegoePkWa/3tXmZ38Rtygq336E9G7mGCULa6/XYYA385\nywarYzH/d/Cvb7Tc7S8eNr8VSbeyc1Hb5Zb81QrHwodgzT/h8Yttkse8/vCvb8K/v2MFvaMn9GDA\nXr9tPjx1TfsPDPUH4JXb4D//79AeUFw366oX47/mMFBR6OUMLsrm91dP4X/OPJp/3noab31rBt+/\nYBwLtuznzhdXYBJl+l76AHx1CVz7NAw/qeW5UWfa3eSOuyJyLG+gXauwc0lk4VzhMCgZbYPN294H\nxCbV27HA+q+nf8mupv74CbvGYfM8e932DyL11pTDhw/Y92//ypbrLJvftgPHB/d1/CS8/UPb3kCD\nM/BGUX/AxlCiObjdDlRb3rGfA02RAWTZU/DKV62L51AwJlLH3tV2LUvJaJh/j7XocvvZp+N1/7Y5\nrIZOh35jI9/7sOk2n9ZFf7DB5xe+ZJ/kB02GSVfB+IsjGXXdyQR7llkLzUswYH8Ptx0Ht1lBL11k\n1zn8cVrEleVaKJv+27KOTfPg2RsjsSWXHQvh18fA1ndh8mdtv0oXRvpfujgSEwkGYNGjdtbbpnnw\n8q22/bPfgrO+Z/+20rKs+3L5P+w1TbWxv9uyFfZvcthJ1ootXQT3TrIWQTSLH7NlK7fbe7RH6WIb\nL6s/EFntv+bV9q/pIlQUUgifTxjRN5cvnj6Kb557LHNXlfGlxxfzzvryji8+VAqHQsnR8ZfPH2Bn\nIlXthGk32Vkt/cfbwWvPSmtBjPuMLbvIcSsMngqTZtkn3I2vR/aV3v4hLHvaDq5PXW3dTjO+a598\nVz5vB7oFf279FFq7L2KpxGL9a7Zd9ftt/bEIhaD+oBWmQZNg1Fkw/96W99ryjh0AHzq7tQ/75Vvs\nk+pjn4Elf4P7psNzN9qB7aO/2DIb34A3fwR/vRDe+KEd0NsS9+YG+Ptl8Oz1tm1718CACXDT63DF\nI/DZZ+HYmfYJe/1rMPpcGzMaONE+nVZut6IA1kV0wg12IO9zFHziVivsVz1uLb/iUXDa12169Dd+\nCL8+umXs4bVvw++Pt8LiDvZZhfDq7fDizTZ29NbPbfLEyh2RvrpU7rTfxaoX4KM/t+zngvutAI+9\n0K6VGTjJ/s415fDoefDQJ63lAbDsSft3dtb3rFXTWG0XbGb3geOvhvGXwKV/hsFT4O1fwt8uhZ8N\nie1OcmMXZ//Avr56uxW7Od+yLrmQY/E21dm/uZLR9vOK5+C/P7UPGpU77e8XbLau0v1b4OFz4MGz\n4D/ftxbncVfC3lX2u0swkrAnxQQxbdo0s2hRO2ahEhfBkOHHr67mtZW7OVDbzFOzT2Lq8D6IxLFv\nQyKY93N4+xf2/Rf+Y5+6Bk6yr/Pvsceve9EGresPWjfHtzZYP+6fpkH+YBujOObTjhskYK/JLobz\nfw0TLrWpGGr32SfAxiqbDPDyh2Dchda//cin7dPxqV+zdZQcA6fcCoOm2Lr/cra1eg5ut9Mvb3od\n+o52BtvVtq2LHo3sK3HCjXaAeuBUGHsBXPlX62p49kY7GNbtgxl32nudcptNL/LoTDjr+7D6JShb\nGfl+Jl5uBS27jx08muvslOLactvXSbPg0z+3QWFfmq3r479bn/mmN20d0//HDqYX/DayyBCsj/vZ\n6621dt2Ldr/tYLOdmlq338YYsgpt2YYqWPAATLkOCgbF/i3/eqF9Yk/PgfyB8MU3bbzhnvHWbTXC\n7hlCbTl88v/ZtQ4NlVZAN8+D075hf/N+4+wU176j7f0+/ptNqTFgvHUpfvVjyC2xbfztWCtY5//K\n1v3B/9kZSIXD7H2GnWTbNOvv8ML/WMG+/lV45jprJcy4o3U/ShfB6z+wLspgMwj2nv4068Jb8Zz9\nTcrXwTdWwR9PsH8/+YPs31mo2f4Nfe45WPxX6xK7YY51mblZhl3GXGBjayuesZkBdi2103rrKmDC\nZXDO3fD7SfCpn8ApX23nP1LbiMhiY8y0DsupKKQ2B2qbuOi++ezYX0+6XyjKyWDswHxuOm0kpx/T\nj+qGZopybEruUMggQmKEIxiA7e/bAXfyZ23WVbD/4T683z7NjpkJ7//JDnaFQ+Bzz9syc/4Xlj5h\nn+yOv9o+bQ+aDFc9ZgOhbirxbR/YQTezwJ7770+t3zm3v3UPmKD1pe/62D7R1VXYwapgSOTJ9dIH\nYeg0G0RuqrWDb/1B6xYAO5BlFcKOD+HKx2DCJXYB3ryf2IH/g/usH/7af8Cfz4jUC9ZlkZkPX1tu\n2/XYZ+CkL9sBcNN/rUhOnw2v3Aq+dLh9hb3XB3+CeT/FjlrGikLRcPtUmZZln4h3LIjEXm58reXs\nsWCzFbPxF9lB/HDZOt9OH+57LDx+kQ1aD5hon/BP+Sq8/0db7uSvwMyf2wF07xprndx7PDRWWkG5\n6I/w/E1WCOsPWHfPrL/b2MZfzrIDLsbeK9gIX54fcWOFQvDE5fZ7u/g++7Dw+0lWTDML4cvvdrxG\nx8vaf9m41FnfgzHn25lYrltn4uXW6nrtO1YwZ9xpk0du/8DuN9Jcb5/2J38WLrnPWkXLnoRL7rdW\n1Y4PI7PqMgvsA8vx18A5P7TCUjjUntvyjhWMWFkI4kBFQYmbnQfreXXZLg7WN7OvupEPt1Sw+2AD\no/vnsbm8lh9fMoGH3t3CxvIaxg8q4FufHsPofnkMK87p7qZHCAXtoFG7D176Mnz6Z7EX1y3+qx1M\nRpxqTfrFj9qn/Iz8iG984xv2P36gHt6421oFx860T3KTrrKzp8pWWfdOY5UdmAcdbwfaouE2DrD1\nHRj1STvlN9AED55p79N/PHxhrn0K/OA+62b57DPWh15/wA4wQ53/txWb7IBqQta6cQfHXx9j23HJ\n/0X6VbYKVr9i6929zLqULv6TbbeIbcOGuVYoPnGrTUlyJNj6nn0ar6uwT8PXPGkHt6VPwunfhL7H\ntCy/cwk8eZV1DV77D/vkXXSUFb2Bx0XiUBvfhKc/awfMEafZ7+acH7Ssq6HKzmYbNcN+XvWi7f+k\nWZGBNl5CIWtJln4UOXbW961VOOU6+/e0YyH843Nw09xIPrDydVZwM/Ot1ZlVCJWl1iXqTRHz35/a\numbcCa/fBZ/68aG5X+NARUHpNFUNzdz46EK2VdSRl+lna0UdxbkZzDpxGC8sKaWsys5++dUVk7hq\n2rBubm0PYfcymPczOO9XkSdUYyKicijsWmoHDDcxYSyMiVhb3U2gyVpSmYXxJTxsrAFM+/0D6zbK\nLLDunCNBKGhFq6rUWkEDJhyZ+3YRKgrKYREMGULGsLe6kT+8sYEvnTGS0f3zqW5oZumOg/z57c18\nsLmCG08ZwWVThzJ2YD4+n2CMYcn2A+RnpXNM/7zui1EoitICFQUlodQ2Bvj+Syv557JdBEKGvnkZ\nnDiimJrGAO9usKknPjGqhAeuO4HC7PRubq2iKCoKyhFhb3UD76zfx/wN5SwrraSmMcCNp44gw+/j\nl/9eS1FOBscPLeKTY/tz2dQhZKX7W1z7w3+upqKmkfMmDuL6U0Z0X0cUpZejoqB0Ox9squDvH25j\nxc5Ktu+vY9pRffj6uceyZncVH+84yMIt+6lqaGZESS5r91Tz3fPGMvuMUepyUpQEoKKgJA3GGF5d\nvptvPLOU5qD9extenMOIvrn876fHMG5QAbc9/TH/Wr6bM47tR0NzkL55GVx0/GA+PWFgp0WivimI\nCC2sE0VJVVQUlKRj675adlc2MLwkhyFFLedaB0OGP/13I49/sJVhxTnsrWpgV2UDQ4qyyUz3cdHx\ngwkEDdkZfiYOKeQTo0pI97cWC1dA1pdVc+OjC6luaOaMY/uRn5XO1885hv4FHWwqpCi9FBUFpUcT\nCIb46/tbWbrjIHurG/loy358AqF2/lxHlOTwlbNGU5yTwdefWUpWup/pI4pZVnqQ8upGSnIz+P6F\n4zl3/ADS/S2nRm7dV4sIHFWSm+CeKUr3oKKg9CoqahopyE6nKRDi/U0VrNrVMk1xyMDrq8tYs9tm\ntTx2QB6P3jg9bJGs3FnJV55Ywvb9dfTLz+SccQMYUpRFdkYab63bG54xNWloIaP65jK8OIdhxTn4\nfUJBVjqnjC6hOWh4eelO+udnMmloERlpPnIy/ORkRObJl1U1sHDrfqaPLKZ/vlolSvKgoqCkHKGQ\n4eMdB1i3p4YLJg1qNRU2GDK8vX4vTy7YzqJtBzjobDo0rDibK08Yht8nzN+wj+3769hdWd/KKvH7\nhGAMU6Uk107HLcnL4JVlu6husHmXstJ9FOdkEDSG7HQ/V504jJLcDPbVNFFW1UBWup+pw/uQmebD\n5xPGDyqgX35mq/rrm4LsqWogK93HwIKsmDGWpkCIjDTNb6m0jYqConRAQ3OQyvpm+udnthpomwIh\ndh206bF3Hqxn8bYD1DUFuXDSIKrqm9m2v47mYIiaxgCby2v5YFMFdU0BjhtaxOzTR7FmdxXlNY1U\n1DTh98Hm8loWbTsQrr8wO536piBNwUgqbxHon59JRpqPDL+PdL+PNL+woayGxoAtl5+VxtiB+eRm\nplGUnU5Gmo+Ptx9kY3kNw/rkMLgoi2DI0BgIUdsYYEifHI7ul8tRxTkMLMymIDuNbRV1bC6vIT8r\nnUlDCzmqJJd9NY1kpfnJz0qjpjHA1opadh2spzloCAQNaX6hIDudESU5jCjJZVBhFmn+1iLU0Bxk\n8bYDvLO+nPVl1fTNy+SU0SUcOyAfnwhpPmFUvzz8viM/w8wd69zfekNZNd9+bjlD+mTzjXOPpSg7\nneLcjF47+01FQVGSjKB4dtUAAAmYSURBVL3VDTQFQhTnZpCTkUZdU4CNe2sIhgxNgRAfbdlP6YF6\nmoIh+y8QojkYYkRJLpOGFlLbGGBdWTXr99TQEAhSUdNEQ3OQ44cVMW5QPhvKajhY30yaT0j3W9dW\n6YF6NpXXUNcUbNGWjDQfTYHO7y2R7heG9smhKCcdAZqDht2VDexzNm5K9wtH98tjb3Uj+2tb7oSX\nne5n/OAC+uVlkpPpp6E5yO7KBpqDIfrnZzGsTzaFORl4h2Zr5ZWzZV8tfXLTKc7NpG9uBsW5GZTk\nZVKSm0Fmuo91e6pZtasKv08Y3S+PE0b0ITcjjRc/LuWjLftJ8/uYPqKYPrkZPL+klNwMP7WNEXHO\nz0rj/ImDOGV0CUeV5NIvP5OQs7o/EDKEQoagsX39cHMFgpCT4advXiYj+uZwVEkuBVlp5Gak4YsS\nvmDIUFXfzMH6ZvwiDCvODgtQQ3OQxkCI/Mw0qhqayc2MuCSj41+dRUVBURTAPiHvr21id2UDlfXN\nDC+2s79qmgKsLK1k58F6+hdk0RQIUVXfTHaGnxEluQwtziYrzR92m1XUNrKtoo5tFbVsdV5dV5nf\nJwzIz2JQURYTBxfyiaNLyM1MIxQyrNpVxc6DdYSMHfxW7Kxk1a4qKuuaqW0KkO73MaQom3S/sLuy\ngdID9dQ0Blr1Y/ygAqaPLOZgXRMVtU1U1DRRUWtFx53qnJ+VxoTBBQjC6t1VVNZbF2H//EzOHjeA\nxkCQBZv3s7uynsumDuV/Z46hpiHAwq37qW0MsmpXFXNW7Ka+Odjq/tGk+wVBWlh7XnIz/ORmppHu\n91HV0Bz+rlzyM9Pok5tBMGTYU9VAMGRiuijTfEJ2up/sDD/fmTmWy084xGR+DkkhCiIyE/g94Ace\nMsb8Iup8JvA4cAJQAcwyxmxtr04VBUVRvBhjqG4MUNcYZEBBxBUYChm2VtSyv7YpPDHApb0YTENz\nkO3769hWUce+mkb8PsEvQppf8ImEJx9MG9GHrHQ/gWCIsupGtu6rZfv+OqobmqlpDFLbGKC2MUBT\nIERBdjpFOekUOq8NzSHW7q6yVoNPGFKUTX5WGgfrminJy6S6oRmfCALUNwepbw7S0Bzk4slDOHlU\nSae+p24XBRHxA+uBc4FSYCFwjTFmtafMV4BJxpgvi8jVwKXGmFnt1auioCiKcujEKwqJnK4wHdho\njNlsjGkCngYujipzMeDucfcccLb01iiPoihKDyCRojAE8GwrRalzLGYZY0wAqARa2UYiMltEFonI\novLyBOwnrCiKogCJFYUuwxjzoDFmmjFmWr9+/bq7OYqiKL2WRIrCTsC7LddQ51jMMiKSBhRiA86K\noihKN5BIUVgIHCMiI0UkA7gaeCWqzCvA9c77K4D/mp42R1ZRFKUXkbDNTY0xARG5FZiLnZL6iDFm\nlYj8CFhkjHkFeBj4m4hsBPZjhUNRFEXpJhK647UxZg4wJ+rYXZ73DcCViWyDoiiKEj89ItCsKIqi\nHBl6XJoLESkHtnXy8r7Avi5sTk8gFfsMqdlv7XNq0Nk+H2WM6XD6Zo8ThcNBRBbFs6KvN5GKfYbU\n7Lf2OTVIdJ/VfaQoiqKEUVFQFEVRwqSaKDzY3Q3oBlKxz5Ca/dY+pwYJ7XNKxRQURVGU9kk1S0FR\nFEVpBxUFRVEUJUzKiIKIzBSRdSKyUUTu6O72JAoR2SoiK0RkqYgsco4Vi8jrIrLBee3T3e08HETk\nERHZKyIrPcdi9lEsf3B+9+UiMrX7Wt552ujz3SKy0/mtl4rI+Z5z33X6vE5EPt09rT48RGSYiMwT\nkdUiskpEvuYc77W/dTt9PnK/tTGm1//D5l7aBIwCMoBlwPjubleC+roV6Bt17FfAHc77O4Bfdnc7\nD7OPZwBTgZUd9RE4H3gNEOBkYEF3t78L+3w38K0YZcc7f+OZwEjnb9/f3X3oRJ8HAVOd9/nYnRzH\n9+bfup0+H7HfOlUshXh2gevNeHe4ewy4pBvbctgYY97BJlD00lYfLwYeN5YPgSIRGXRkWtp1tNHn\ntrgYeNoY02iM2QJsxP4f6FEYY3YbY5Y476uBNdiNuXrtb91On9uiy3/rVBGFeHaB6y0Y4D8islhE\nZjvHBhhjdjvv9wADuqdpCaWtPvb23/5Wx1XyiMct2Ov6LCIjgCnAAlLkt47qMxyh3zpVRCGVOM0Y\nMxU4D7hFRM7wnjTW5uzV85BToY8O9wNHA5OB3cBvu7c5iUFE8oDngduNMVXec731t47R5yP2W6eK\nKMSzC1yvwBiz03ndC7yINSXLXDPaed3bfS1MGG31sdf+9saYMmNM0BgTAv5CxG3Qa/osIunYwfEJ\nY8wLzuFe/VvH6vOR/K1TRRTi2QWuxyMiuSKS774HPgWspOUOd9cDL3dPCxNKW318Bfi8MzPlZKDS\n43ro0UT5yy/F/tZg+3y1iGSKyEjgGOCjI92+w0VEBLsR1xpjzD2eU732t26rz0f0t+7uaPsRjOqf\nj43kbwK+193tSVAfR2FnIiwDVrn9BEqAN4ENwBtAcXe39TD7+RTWhG7G+lBvaquP2Jko9zm/+wpg\nWne3vwv7/DenT8udwWGQp/z3nD6vA87r7vZ3ss+nYV1Dy4Glzr/ze/Nv3U6fj9hvrWkuFEVRlDCp\n4j5SFEVR4kBFQVEURQmjoqAoiqKEUVFQFEVRwqgoKIqiKGFUFBTlCCIiM0Tk1e5uh6K0hYqCoiiK\nEkZFQVFiICKfE5GPnNz1fxYRv4jUiMjvnDz3b4pIP6fsZBH50ElW9qInv/9oEXlDRJaJyBIROdqp\nPk9EnhORtSLyhLOKVVGSAhUFRYlCRMYBs4BTjTGTgSDwWSAXWGSMmQC8DfzAueRx4DvGmEnYVafu\n8SeA+4wxxwOnYFckg818eTs2F/4o4NSEd0pR4iStuxugKEnI2cAJwELnIT4bm3QtBPzDKfN34AUR\nKQSKjDFvO8cfA551clANMca8CGCMaQBw6vvIGFPqfF4KjADmJ75bitIxKgqK0hoBHjPGfLfFQZH/\nF1WuszliGj3vg+j/QyWJUPeRorTmTeAKEekP4T2Bj8L+f7nCKXMtMN8YUwkcEJHTnePXAW8bu2tW\nqYhc4tSRKSI5R7QXitIJ9AlFUaIwxqwWke9jd7DzYTOT3gLUAtOdc3uxcQew6ZsfcAb9zcCNzvHr\ngD+LyI+cOq48gt1QlE6hWVIVJU5EpMYYk9fd7VCURKLuI0VRFCWMWgqKoihKGLUUFEVRlDAqCoqi\nKEoYFQVFURQljIqCoiiKEkZFQVEURQnz/wEUn1mj8PawxAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}