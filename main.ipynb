{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnansary/pytfBusterNet/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDeBMUvvnMLq",
        "colab_type": "text"
      },
      "source": [
        "# colab specific task\n",
        "*   mount google drive\n",
        "*   change working directory to git repo\n",
        "*   Check TF version\n",
        "*    TPU check\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI51ZAMAnFBD",
        "colab_type": "code",
        "outputId": "6c6e19c4-3360-4488-a57f-35927521b75a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHmLeloUn1AN",
        "colab_type": "code",
        "outputId": "3efe2c5d-c2f5-47e6-8564-063b58001ad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/gdrive/My\\ Drive/CopyMove/pytfBusterNet/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/CopyMove/pytfBusterNet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cuya7eeNoAJP",
        "colab_type": "code",
        "outputId": "3f16895b-2954-459d-e428-3963821884d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!pip3 install tensorflow==1.13.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.16.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.33.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.15.5)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (3.0.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (41.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqyJDiJBBX1E",
        "colab_type": "text"
      },
      "source": [
        "## TPU Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtxuxRffBHmZ",
        "colab_type": "code",
        "outputId": "cff8a770-11df-4df1-87f5-39babed6ca05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "\n",
        "  with tf.Session(tpu_address) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(devices)\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.110.208.90:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 13849900095548455401),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7087578239193037491),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9374415105526213586),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 12823212531507121852),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 10834216392966996874),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 10393830486862736608),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 16782088074026870289),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 11914229742950451560),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 17581914393295718580),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 8621953355763316168),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 10840351006991694450)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Wq9KaIBBBel",
        "colab_type": "text"
      },
      "source": [
        "# Manipulation Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niex3kEVBGIL",
        "colab_type": "text"
      },
      "source": [
        "## IMPORT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2pdzT7bCphW",
        "colab_type": "code",
        "outputId": "2053c8bf-f92b-409f-bc55-9499f9c19101",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# imports for man_net and loading\n",
        "from BusterNet.models import man_net\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# read data\n",
        "def readh5(d_path):\n",
        "    data=h5py.File(d_path, 'r')\n",
        "    data = np.array(data['data'])\n",
        "    return data\n",
        "\n",
        "# load data\n",
        "d_path=os.path.join(os.getcwd(),'DataSet')\n",
        "Xp=os.path.join(d_path,'X.h5')\n",
        "Ymp=os.path.join(d_path,'Ym.h5')\n",
        "\n",
        "X=readh5(Xp)\n",
        "Ym=readh5(Ymp)\n",
        "print(X.shape)\n",
        "print(Ym.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(672, 256, 256, 3)\n",
            "(672, 256, 256, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVqj6bhJFA83",
        "colab_type": "text"
      },
      "source": [
        "## Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLu8k7MdFAg6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "261920f5-ab1b-433c-84c1-3a058e03fa58"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "model=man_net()\n",
        "model.summary()\n",
        "model.compile(optimizer=Adam(lr=0.01), loss=tf.keras.losses.binary_crossentropy)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 256, 256, 64) 1792        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 256, 256, 64) 36928       conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 128, 128, 64) 0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 128, 128, 128 73856       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 128, 128, 128 147584      conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 128)  0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 64, 64, 256)  295168      max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 64, 64, 256)  590080      conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 64, 64, 256)  590080      conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 256)  0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 512)  1180160     max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 512)  2359808     conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 512)  2359808     conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 512)  0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 8)    4104        max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 8)    36872       max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 8)    102408      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 16, 16, 24)   0           conv2d_10[0][0]                  \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1 (BatchNo (None, 16, 16, 24)   96          concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 16, 16, 24)   0           batch_normalization_v1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 32, 32, 24)   0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 6)    150         lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 6)    1302        lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 32, 32, 6)    3606        lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 18)   0           conv2d_13[0][0]                  \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_1 (Batch (None, 32, 32, 18)   72          concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 18)   0           batch_normalization_v1_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 64, 64, 18)   0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 64, 64, 4)    76          lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 64, 64, 4)    652         lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 64, 64, 4)    1804        lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 64, 64, 12)   0           conv2d_16[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_2 (Batch (None, 64, 64, 12)   48          concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 64, 64, 12)   0           batch_normalization_v1_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 128, 128, 12) 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 128, 128, 2)  26          lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 128, 128, 2)  218         lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 128, 128, 2)  602         lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 128, 128, 6)  0           conv2d_19[0][0]                  \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_3 (Batch (None, 128, 128, 6)  24          concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 128, 128, 6)  0           batch_normalization_v1_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 256, 256, 6)  0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 256, 256, 2)  302         lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 256, 256, 2)  590         lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 256, 256, 2)  1454        lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 256, 256, 6)  0           conv2d_22[0][0]                  \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_4 (Batch (None, 256, 256, 6)  24          concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 256, 256, 6)  0           batch_normalization_v1_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 256, 256, 1)  55          activation_4[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 7,789,749\n",
            "Trainable params: 7,789,617\n",
            "Non-trainable params: 132\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiHwyIDQGqVB",
        "colab_type": "text"
      },
      "source": [
        "## Convert Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZqfpN7wGp5X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "d864fc20-7440-4020-a4b9-8a949c58a5eb"
      },
      "source": [
        "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "def convert_model_TPU(model):\n",
        "  return tf.contrib.tpu.keras_to_tpu_model(model,strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "model=convert_model_TPU(model)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.110.208.90:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 13849900095548455401)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7087578239193037491)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9374415105526213586)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 12823212531507121852)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 10834216392966996874)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 10393830486862736608)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 16782088074026870289)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 11914229742950451560)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 17581914393295718580)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 8621953355763316168)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 10840351006991694450)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFwnouy4HESm",
        "colab_type": "text"
      },
      "source": [
        "## Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccZissONHHRk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3542686e-75be-49c3-fa17-2b2b0a778adf"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "epochs=250\n",
        "batch_size=30\n",
        "Xt,Xv,Yt,Yv = train_test_split(X,Ym, test_size=0.2)\n",
        "print(Xt.shape)\n",
        "print(Xv.shape)\n",
        "print(Yt.shape)\n",
        "print(Yv.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(537, 256, 256, 3)\n",
            "(135, 256, 256, 3)\n",
            "(537, 256, 256, 1)\n",
            "(135, 256, 256, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyJkGjcbHXmp",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3hCZ-7fHbWQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7efdd5f-a72a-4901-d165-bd16a8533456"
      },
      "source": [
        "history=model.fit(Xt,Yt,validation_data=(Xv,Yv),epochs=epochs,batch_size=batch_size, verbose=1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 537 samples, validate on 135 samples\n",
            "Epoch 1/250\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(3,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(3, 256, 256, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(3, 256, 256, 1), dtype=tf.float32, name='conv2d_25_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f1d34f6b518> []\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 52.407920837402344 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "INFO:tensorflow:CPU -> TPU lr: 0.009999999776482582 {0.01}\n",
            "INFO:tensorflow:CPU -> TPU beta_1: 0.8999999761581421 {0.9}\n",
            "INFO:tensorflow:CPU -> TPU beta_2: 0.9990000128746033 {0.999}\n",
            "INFO:tensorflow:CPU -> TPU decay: 0.0 {0.0}\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "510/537 [===========================>..] - ETA: 4s - loss: 0.3716 INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(3,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(3, 256, 256, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(3, 256, 256, 1), dtype=tf.float32, name='conv2d_25_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.009999999776482582, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f1d2a60f278> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 33.15455651283264 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(1,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(1, 256, 256, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(1, 256, 256, 1), dtype=tf.float32, name='conv2d_25_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f1d2a60f278> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 14.03532099723816 secs\n",
            "537/537 [==============================] - 142s 264ms/sample - loss: 0.3571 - val_loss: 0.3011\n",
            "Epoch 2/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0798 - val_loss: 0.3013\n",
            "Epoch 3/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0816 - val_loss: 0.3011\n",
            "Epoch 4/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0822 - val_loss: 0.3012\n",
            "Epoch 5/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0772 - val_loss: 0.2863\n",
            "Epoch 6/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0772 - val_loss: 0.1896\n",
            "Epoch 7/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0755 - val_loss: 0.1688\n",
            "Epoch 8/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0763 - val_loss: 0.0925\n",
            "Epoch 9/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0773 - val_loss: 0.1020\n",
            "Epoch 10/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0754 - val_loss: 0.1019\n",
            "Epoch 11/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0772 - val_loss: 0.1127\n",
            "Epoch 12/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0754 - val_loss: 0.0955\n",
            "Epoch 13/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0757 - val_loss: 0.1269\n",
            "Epoch 14/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0744 - val_loss: 0.1194\n",
            "Epoch 15/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0731 - val_loss: 0.0813\n",
            "Epoch 16/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0714 - val_loss: 0.0823\n",
            "Epoch 17/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0733 - val_loss: 0.1191\n",
            "Epoch 18/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0750 - val_loss: 0.0837\n",
            "Epoch 19/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0744 - val_loss: 0.0957\n",
            "Epoch 20/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0702 - val_loss: 0.1020\n",
            "Epoch 21/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0721 - val_loss: 0.1324\n",
            "Epoch 22/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0709 - val_loss: 0.1126\n",
            "Epoch 23/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0694 - val_loss: 0.0915\n",
            "Epoch 24/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0727 - val_loss: 0.1370\n",
            "Epoch 25/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0694 - val_loss: 0.1016\n",
            "Epoch 26/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0689 - val_loss: 0.1102\n",
            "Epoch 27/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0706 - val_loss: 0.1000\n",
            "Epoch 28/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0650 - val_loss: 0.1193\n",
            "Epoch 29/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0677 - val_loss: 0.1514\n",
            "Epoch 30/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0620 - val_loss: 0.0962\n",
            "Epoch 31/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0613 - val_loss: 0.0906\n",
            "Epoch 32/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0625 - val_loss: 0.1888\n",
            "Epoch 33/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0601 - val_loss: 0.2106\n",
            "Epoch 34/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0514 - val_loss: 0.1434\n",
            "Epoch 35/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0532 - val_loss: 0.0738\n",
            "Epoch 36/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0521 - val_loss: 0.1335\n",
            "Epoch 37/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0479 - val_loss: 0.0718\n",
            "Epoch 38/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0451 - val_loss: 0.0912\n",
            "Epoch 39/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0390 - val_loss: 0.1141\n",
            "Epoch 40/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0417 - val_loss: 0.0917\n",
            "Epoch 41/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0431 - val_loss: 0.1014\n",
            "Epoch 42/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0392 - val_loss: 0.0559\n",
            "Epoch 43/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0352 - val_loss: 0.0449\n",
            "Epoch 44/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0349 - val_loss: 0.0827\n",
            "Epoch 45/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0339 - val_loss: 0.0845\n",
            "Epoch 46/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0350 - val_loss: 0.0612\n",
            "Epoch 47/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0366 - val_loss: 0.0686\n",
            "Epoch 48/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0343 - val_loss: 0.0723\n",
            "Epoch 49/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0314 - val_loss: 0.0417\n",
            "Epoch 50/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0292 - val_loss: 0.0368\n",
            "Epoch 51/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0270 - val_loss: 0.0828\n",
            "Epoch 52/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0279 - val_loss: 0.0531\n",
            "Epoch 53/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0264 - val_loss: 0.0532\n",
            "Epoch 54/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0277 - val_loss: 0.0715\n",
            "Epoch 55/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0275 - val_loss: 0.0461\n",
            "Epoch 56/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0265 - val_loss: 0.0414\n",
            "Epoch 57/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0255 - val_loss: 0.0570\n",
            "Epoch 58/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0250 - val_loss: 0.0613\n",
            "Epoch 59/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0253 - val_loss: 0.0802\n",
            "Epoch 60/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0239 - val_loss: 0.0464\n",
            "Epoch 61/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0247 - val_loss: 0.0747\n",
            "Epoch 62/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0242 - val_loss: 0.0415\n",
            "Epoch 63/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0216 - val_loss: 0.0518\n",
            "Epoch 64/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0228 - val_loss: 0.0491\n",
            "Epoch 65/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0222 - val_loss: 0.0391\n",
            "Epoch 66/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0224 - val_loss: 0.0357\n",
            "Epoch 67/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0208 - val_loss: 0.0388\n",
            "Epoch 68/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0204 - val_loss: 0.0540\n",
            "Epoch 69/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0204 - val_loss: 0.0324\n",
            "Epoch 70/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0198 - val_loss: 0.0373\n",
            "Epoch 71/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0194 - val_loss: 0.0319\n",
            "Epoch 72/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0188 - val_loss: 0.0300\n",
            "Epoch 73/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0191 - val_loss: 0.0360\n",
            "Epoch 74/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0194 - val_loss: 0.0480\n",
            "Epoch 75/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0188 - val_loss: 0.0337\n",
            "Epoch 76/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0177 - val_loss: 0.0320\n",
            "Epoch 77/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0173 - val_loss: 0.0306\n",
            "Epoch 78/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0181 - val_loss: 0.0440\n",
            "Epoch 79/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0184 - val_loss: 0.0512\n",
            "Epoch 80/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0172 - val_loss: 0.0462\n",
            "Epoch 81/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0170 - val_loss: 0.0438\n",
            "Epoch 82/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0171 - val_loss: 0.0616\n",
            "Epoch 83/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0176 - val_loss: 0.0305\n",
            "Epoch 84/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0177 - val_loss: 0.0373\n",
            "Epoch 85/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0183 - val_loss: 0.0516\n",
            "Epoch 86/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0179 - val_loss: 0.0332\n",
            "Epoch 87/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0158 - val_loss: 0.0324\n",
            "Epoch 88/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0160 - val_loss: 0.0332\n",
            "Epoch 89/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0158 - val_loss: 0.0446\n",
            "Epoch 90/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0170 - val_loss: 0.0348\n",
            "Epoch 91/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0158 - val_loss: 0.0312\n",
            "Epoch 92/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0155 - val_loss: 0.0326\n",
            "Epoch 93/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0144 - val_loss: 0.0399\n",
            "Epoch 94/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0146 - val_loss: 0.0305\n",
            "Epoch 95/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0151 - val_loss: 0.0349\n",
            "Epoch 96/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0142 - val_loss: 0.0583\n",
            "Epoch 97/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0161 - val_loss: 0.0463\n",
            "Epoch 98/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0147 - val_loss: 0.0270\n",
            "Epoch 99/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0151 - val_loss: 0.0390\n",
            "Epoch 100/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0149 - val_loss: 0.0307\n",
            "Epoch 101/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0138 - val_loss: 0.0290\n",
            "Epoch 102/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0133 - val_loss: 0.0304\n",
            "Epoch 103/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0130 - val_loss: 0.0337\n",
            "Epoch 104/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0125 - val_loss: 0.0337\n",
            "Epoch 105/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0139 - val_loss: 0.0286\n",
            "Epoch 106/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0134 - val_loss: 0.0315\n",
            "Epoch 107/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0128 - val_loss: 0.0318\n",
            "Epoch 108/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0128 - val_loss: 0.0490\n",
            "Epoch 109/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0132 - val_loss: 0.0289\n",
            "Epoch 110/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0133 - val_loss: 0.0299\n",
            "Epoch 111/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0125 - val_loss: 0.0369\n",
            "Epoch 112/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0120 - val_loss: 0.0344\n",
            "Epoch 113/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0122 - val_loss: 0.0327\n",
            "Epoch 114/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0120 - val_loss: 0.0457\n",
            "Epoch 115/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0122 - val_loss: 0.0346\n",
            "Epoch 116/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0117 - val_loss: 0.0289\n",
            "Epoch 117/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0117 - val_loss: 0.0272\n",
            "Epoch 118/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0124 - val_loss: 0.0298\n",
            "Epoch 119/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0119 - val_loss: 0.0314\n",
            "Epoch 120/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0117 - val_loss: 0.0384\n",
            "Epoch 121/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0110 - val_loss: 0.0294\n",
            "Epoch 122/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0112 - val_loss: 0.0262\n",
            "Epoch 123/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0109 - val_loss: 0.0296\n",
            "Epoch 124/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0107 - val_loss: 0.0265\n",
            "Epoch 125/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0106 - val_loss: 0.0288\n",
            "Epoch 126/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0107 - val_loss: 0.0348\n",
            "Epoch 127/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0112 - val_loss: 0.0313\n",
            "Epoch 128/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0116 - val_loss: 0.0404\n",
            "Epoch 129/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0111 - val_loss: 0.0443\n",
            "Epoch 130/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0117 - val_loss: 0.0334\n",
            "Epoch 131/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0114 - val_loss: 0.0332\n",
            "Epoch 132/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0114 - val_loss: 0.0453\n",
            "Epoch 133/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0121 - val_loss: 0.0353\n",
            "Epoch 134/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0133 - val_loss: 0.0365\n",
            "Epoch 135/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0129 - val_loss: 0.0372\n",
            "Epoch 136/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0117 - val_loss: 0.0364\n",
            "Epoch 137/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0101 - val_loss: 0.0457\n",
            "Epoch 138/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0096 - val_loss: 0.0287\n",
            "Epoch 139/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0093 - val_loss: 0.0344\n",
            "Epoch 140/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0098 - val_loss: 0.0383\n",
            "Epoch 141/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0102 - val_loss: 0.0355\n",
            "Epoch 142/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0102 - val_loss: 0.0262\n",
            "Epoch 143/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0093 - val_loss: 0.0324\n",
            "Epoch 144/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0093 - val_loss: 0.0270\n",
            "Epoch 145/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0092 - val_loss: 0.0332\n",
            "Epoch 146/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0087 - val_loss: 0.0310\n",
            "Epoch 147/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0093 - val_loss: 0.0278\n",
            "Epoch 148/250\n",
            "537/537 [==============================] - 11s 20ms/sample - loss: 0.0091 - val_loss: 0.0364\n",
            "Epoch 149/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0086 - val_loss: 0.0355\n",
            "Epoch 150/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0090 - val_loss: 0.0270\n",
            "Epoch 151/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0095 - val_loss: 0.0290\n",
            "Epoch 152/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0086 - val_loss: 0.0292\n",
            "Epoch 153/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0085 - val_loss: 0.0338\n",
            "Epoch 154/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0083 - val_loss: 0.0275\n",
            "Epoch 155/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0084 - val_loss: 0.0297\n",
            "Epoch 156/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0087 - val_loss: 0.0573\n",
            "Epoch 157/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0086 - val_loss: 0.0280\n",
            "Epoch 158/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0087 - val_loss: 0.0349\n",
            "Epoch 159/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0089 - val_loss: 0.0274\n",
            "Epoch 160/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0088 - val_loss: 0.0378\n",
            "Epoch 161/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0089 - val_loss: 0.0331\n",
            "Epoch 162/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0104 - val_loss: 0.0561\n",
            "Epoch 163/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0089 - val_loss: 0.0365\n",
            "Epoch 164/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0083 - val_loss: 0.0296\n",
            "Epoch 165/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0083 - val_loss: 0.0410\n",
            "Epoch 166/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0081 - val_loss: 0.0285\n",
            "Epoch 167/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0083 - val_loss: 0.0313\n",
            "Epoch 168/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0090 - val_loss: 0.0297\n",
            "Epoch 169/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0093 - val_loss: 0.0378\n",
            "Epoch 170/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0091 - val_loss: 0.0336\n",
            "Epoch 171/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0083 - val_loss: 0.0318\n",
            "Epoch 172/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0082 - val_loss: 0.0332\n",
            "Epoch 173/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0082 - val_loss: 0.0300\n",
            "Epoch 174/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0078 - val_loss: 0.0286\n",
            "Epoch 175/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0080 - val_loss: 0.0305\n",
            "Epoch 176/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0081 - val_loss: 0.0290\n",
            "Epoch 177/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0083 - val_loss: 0.0274\n",
            "Epoch 178/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0078 - val_loss: 0.0258\n",
            "Epoch 179/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0078 - val_loss: 0.0345\n",
            "Epoch 180/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0079 - val_loss: 0.0267\n",
            "Epoch 181/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0075 - val_loss: 0.0264\n",
            "Epoch 182/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0076 - val_loss: 0.0327\n",
            "Epoch 183/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0076 - val_loss: 0.0332\n",
            "Epoch 184/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0079 - val_loss: 0.0271\n",
            "Epoch 185/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0081 - val_loss: 0.0282\n",
            "Epoch 186/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0079 - val_loss: 0.0306\n",
            "Epoch 187/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0079 - val_loss: 0.0308\n",
            "Epoch 188/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0080 - val_loss: 0.0256\n",
            "Epoch 189/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0083 - val_loss: 0.0277\n",
            "Epoch 190/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0090 - val_loss: 0.0406\n",
            "Epoch 191/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0081 - val_loss: 0.0347\n",
            "Epoch 192/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0076 - val_loss: 0.0302\n",
            "Epoch 193/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0076 - val_loss: 0.0346\n",
            "Epoch 194/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0078 - val_loss: 0.0316\n",
            "Epoch 195/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0076 - val_loss: 0.0348\n",
            "Epoch 196/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0078 - val_loss: 0.0275\n",
            "Epoch 197/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0074 - val_loss: 0.0284\n",
            "Epoch 198/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0073 - val_loss: 0.0311\n",
            "Epoch 199/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0072 - val_loss: 0.0326\n",
            "Epoch 200/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0072 - val_loss: 0.0442\n",
            "Epoch 201/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0070 - val_loss: 0.0319\n",
            "Epoch 202/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0072 - val_loss: 0.0322\n",
            "Epoch 203/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0071 - val_loss: 0.0339\n",
            "Epoch 204/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0070 - val_loss: 0.0419\n",
            "Epoch 205/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0074 - val_loss: 0.0303\n",
            "Epoch 206/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0074 - val_loss: 0.0255\n",
            "Epoch 207/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0073 - val_loss: 0.0273\n",
            "Epoch 208/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0069 - val_loss: 0.0285\n",
            "Epoch 209/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0068 - val_loss: 0.0291\n",
            "Epoch 210/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0068 - val_loss: 0.0273\n",
            "Epoch 211/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0066 - val_loss: 0.0325\n",
            "Epoch 212/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0065 - val_loss: 0.0277\n",
            "Epoch 213/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0066 - val_loss: 0.0377\n",
            "Epoch 214/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0067 - val_loss: 0.0339\n",
            "Epoch 215/250\n",
            "537/537 [==============================] - 12s 21ms/sample - loss: 0.0067 - val_loss: 0.0315\n",
            "Epoch 216/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0071 - val_loss: 0.0310\n",
            "Epoch 217/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0069 - val_loss: 0.0331\n",
            "Epoch 218/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0068 - val_loss: 0.0320\n",
            "Epoch 219/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0069 - val_loss: 0.0384\n",
            "Epoch 220/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0068 - val_loss: 0.0331\n",
            "Epoch 221/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0070 - val_loss: 0.0418\n",
            "Epoch 222/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0068 - val_loss: 0.0346\n",
            "Epoch 223/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0066 - val_loss: 0.0333\n",
            "Epoch 224/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0065 - val_loss: 0.0307\n",
            "Epoch 225/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0065 - val_loss: 0.0321\n",
            "Epoch 226/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0065 - val_loss: 0.0282\n",
            "Epoch 227/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0068 - val_loss: 0.0353\n",
            "Epoch 228/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0067 - val_loss: 0.0393\n",
            "Epoch 229/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0065 - val_loss: 0.0297\n",
            "Epoch 230/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0064 - val_loss: 0.0382\n",
            "Epoch 231/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0063 - val_loss: 0.0313\n",
            "Epoch 232/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0062 - val_loss: 0.0369\n",
            "Epoch 233/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0068 - val_loss: 0.0329\n",
            "Epoch 234/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0071 - val_loss: 0.0305\n",
            "Epoch 235/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0070 - val_loss: 0.0321\n",
            "Epoch 236/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0068 - val_loss: 0.0367\n",
            "Epoch 237/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0066 - val_loss: 0.0308\n",
            "Epoch 238/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0070 - val_loss: 0.0377\n",
            "Epoch 239/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0071 - val_loss: 0.0314\n",
            "Epoch 240/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0069 - val_loss: 0.0345\n",
            "Epoch 241/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0065 - val_loss: 0.0359\n",
            "Epoch 242/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0063 - val_loss: 0.0369\n",
            "Epoch 243/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0062 - val_loss: 0.0284\n",
            "Epoch 244/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0061 - val_loss: 0.0296\n",
            "Epoch 245/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0064 - val_loss: 0.0301\n",
            "Epoch 246/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0063 - val_loss: 0.0290\n",
            "Epoch 247/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0061 - val_loss: 0.0310\n",
            "Epoch 248/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0060 - val_loss: 0.0281\n",
            "Epoch 249/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0060 - val_loss: 0.0278\n",
            "Epoch 250/250\n",
            "537/537 [==============================] - 11s 21ms/sample - loss: 0.0061 - val_loss: 0.0330\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR1wDAzTNiKx",
        "colab_type": "text"
      },
      "source": [
        "## Tarining History"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVzatlv9NlqY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "ce34969e-e63b-4061-9dab-3926fbb277f4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1d2948e860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYWZmhVwszMyskIuFmZkV+geYi7Mw3fTznwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYPPJcGgcndo",
        "colab_type": "text"
      },
      "source": [
        "## Save Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS8O9s1acmCw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d87b51ea-1305-4072-c73c-73efc0cbf666"
      },
      "source": [
        "model.save_weights('man_net.h5')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.009999999776482582\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}